{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Guidance: Noisy Prices and Return-based Anomalies in Corporate Bonds\n",
        "- Task: Replicate Table 1\n",
        "- Data used: FINRA TRACE\n",
        "- Available code and data: https://openbondassetpricing.com/code/"
      ],
      "metadata": {
        "id": "x41myQTIR90w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Bond Data Processing for Price, Volume, and Illiquidity Analysis\n",
        "This Python script is designed to streamline the process of extracting, cleaning, and analyzing bond data from the WRDS TRACE database. The script focuses on three key areas of bond market analysis: price, volume, and illiquidity metrics. By leveraging data directly from WRDS, the script ensures access to comprehensive and accurate bond trading information, enabling a detailed examination of market dynamics."
      ],
      "metadata": {
        "id": "TF1aEEJJTJfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization and Environment Setup\n",
        "This section imports necessary libraries and sets up pandas options to suppress warnings, defining paths for data output and storage."
      ],
      "metadata": {
        "id": "ad6vZuLtUVud"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKunI3EGRj-0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wrds\n",
        "from itertools import chain\n",
        "import datetime as dt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import config\n",
        "from pathlib import Path\n",
        "OUTPUT_DIR = Path(config.OUTPUT_DIR)\n",
        "DATA_DIR = Path(config.DATA_DIR)\n",
        "WRDS_USERNAME = config.WRDS_USERNAME"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries like pandas and numpy are imported for data manipulation, while wrds is used for database access. Warning suppression and directory paths are set up for efficient data handling."
      ],
      "metadata": {
        "id": "NWDwQsMTUd9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting to WRDS Database\n",
        "This snippet establishes a connection to the WRDS database using the username provided in the configuration file."
      ],
      "metadata": {
        "id": "tcpJoXhSUm3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = wrds.Connection(wrds_username=WRDS_USERNAME)"
      ],
      "metadata": {
        "id": "dzqywK_iUpXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The connection to the WRDS database is essential for querying and downloading bond data directly from their platform."
      ],
      "metadata": {
        "id": "UiVm0nswUqyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and Processing Mergent Files\n",
        "Queries the WRDS database to download issuer and issue data, merging them based on issuer_id."
      ],
      "metadata": {
        "id": "8-uJ5Zt2UtW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fisd_issuer = db.raw_sql(\"\"\"SELECT issuer_id,country_domicile FROM fisd.fisd_mergedissuer\"\"\")\n",
        "fisd_issue = db.raw_sql(\"\"\"SELECT ... FROM fisd.fisd_mergedissue\"\"\")\n",
        "fisd = pd.merge(fisd_issue, fisd_issuer, on=['issuer_id'], how=\"left\")"
      ],
      "metadata": {
        "id": "9kw3wL4uUwr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step retrieves and combines issuer and issue data from WRDS, creating a comprehensive dataset for bond analysis."
      ],
      "metadata": {
        "id": "qe0DVsmnUyQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering Bond Data\n",
        "Applies filters to the merged dataset to retain bonds meeting specific criteria, such as being US-based and non-convertible."
      ],
      "metadata": {
        "id": "Citam4O2UzvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fisd = fisd[(fisd.country_domicile == 'USA') & (fisd.convertible == 'N')]\n",
        "...\n",
        "fisd = fisd[~fisd.dated_date.isnull()]"
      ],
      "metadata": {
        "id": "aqj3g9HbU5Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filters are used to refine the bond dataset, focusing on bonds of interest based on predefined criteria."
      ],
      "metadata": {
        "id": "x5CvMdw9U6TI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the Processed FISD Data\n",
        "The filtered dataset is saved to a specified directory for future use, ensuring the processed data is accessible for analysis or further processing."
      ],
      "metadata": {
        "id": "gdgUALGnU8Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = './' / Path(DATA_DIR) / \"pulled\" / \"fisd.csv\"\n",
        "fisd.to_csv(path, index=False)"
      ],
      "metadata": {
        "id": "6sP63NvCU_nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The processed and filtered bond data is stored in a CSV file, making it readily available for subsequent analysis or steps in the processing pipeline."
      ],
      "metadata": {
        "id": "312tsQzRVBWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing and Cleaning Data for Analysis\n",
        "Additional preparation and cleaning steps are undertaken, including ensuring unique identifiers for bonds and dividing the dataset into manageable chunks for processing."
      ],
      "metadata": {
        "id": "28M7NGZ5VC3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CUSIP_Sample = list(fisd['complete_cusip'].unique())\n",
        "cusip_chunks = list(divide_chunks(CUSIP_Sample, 500))"
      ],
      "metadata": {
        "id": "aFYvlGK6VIIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section involves further preparation of the bond data, including deduplication and segmentation into smaller chunks for efficient processing."
      ],
      "metadata": {
        "id": "ng1FItpZVKvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterative Data Cleaning and Analysis for Prices, Volume, and Illiquidity\n",
        "Processes each data chunk to calculate daily prices, volumes, and illiquidity metrics."
      ],
      "metadata": {
        "id": "2xKz008FVMB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(cusip_chunks)):\n",
        "    trace = db.raw_sql('SELECT ... FROM trace.trace_enhanced WHERE cusip_id in %(cusip_id)s', params=parm)\n",
        "    ...\n",
        "    # Calculate Equal-Weighted Prices\n",
        "    prc_EW = trace.groupby(['cusip_id', 'trd_exctn_dt'])['rptd_pr'].mean().to_frame(name='prc_ew')\n",
        "    # Calculate Volume-Weighted Prices\n",
        "    trace['dollar_vol'] = trace['entrd_vol_qt'] * trace['rptd_pr'] / 100\n",
        "    prc_VW = trace.groupby(['cusip_id', 'trd_exctn_dt']).apply(lambda x: np.sum(x['rptd_pr'] * (x['entrd_vol_qt'] / x['entrd_vol_qt'].sum()))).to_frame(name='prc_vw')\n",
        "    # Calculate Volumes\n",
        "    VolumesAll = trace.groupby(['cusip_id', 'trd_exctn_dt'])['entrd_vol_qt'].sum().to_frame(name='volume')\n",
        "    # Calculate Illiquidity Measures\n",
        "    bid_ask_spread = calculate_bid_ask_spread(trace)"
      ],
      "metadata": {
        "id": "OgOKXXpEVS0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section performs comprehensive data cleaning and calculates key financial metrics, including prices (equal-weighted and volume-weighted), trading volumes, and illiquidity measures such as the bid-ask spread."
      ],
      "metadata": {
        "id": "J_t_922XXWQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Final Analysis Results\n",
        "Stores the calculated prices, volumes, and illiquidity metrics for further analysis or reporting."
      ],
      "metadata": {
        "id": "2g9-SlBIVVlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PricesExport.to_csv('./' / Path(DATA_DIR) / \"pulled\" / 'Prices.csv.gzip', compression='gzip')\n",
        "VolumeExport.to_csv('./' / Path(DATA_DIR) / \"pulled\" / 'Volumes.csv.gzip', compression='gzip')\n",
        "IlliqExport.to_csv('./' / Path(DATA_DIR) / \"pulled\" / 'Illiq.csv.gzip', compression='gzip')"
      ],
      "metadata": {
        "id": "lh6gIb3MVY7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code saves the analysis results in compressed GZIP format, facilitating efficient storage and access. Through this comprehensive approach, the process not only provides insights into daily bond prices and trading volumes but also evaluates market liquidity, offering a deep dive into the dynamics of the bond market."
      ],
      "metadata": {
        "id": "hqW3jV_5Vax8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Converting WRDS Bond Ratings to Categorical Scores\n",
        "This Python script automates fetching bond ratings from the WRDS (Wharton Research Data Services) database, specifically targeting Moody's and Standard & Poor's (S&P) ratings. Instead of converting these ratings into numeric scores, it categorizes them into three broad quality categories: 'A and above', 'BBB', or 'Junk'. The script then cleans and saves the processed data for subsequent analysis."
      ],
      "metadata": {
        "id": "aGOfgoCUXjyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization and Setup\n",
        "The script begins by importing necessary libraries, suppressing warnings, and setting up directories for output."
      ],
      "metadata": {
        "id": "Ix-OKUt7ZDjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from dateutil.relativedelta import *\n",
        "from pandas.tseries.offsets import *\n",
        "import datetime as datetime\n",
        "import wrds\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import config\n",
        "from pathlib import Path\n",
        "OUTPUT_DIR = Path(config.OUTPUT_DIR)\n",
        "DATA_DIR = Path(config.DATA_DIR)\n",
        "WRDS_USERNAME = config.WRDS_USERNAME"
      ],
      "metadata": {
        "id": "g0HFU16lZGCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ratings Conversion Mappings\n",
        "Although mappings for S&P and Moody's ratings to numeric values are defined, the primary goal is to categorize these ratings into qualitative buckets."
      ],
      "metadata": {
        "id": "O3BlIoNqZHiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp_rating_mapping = { \"AAA\": 1, \"AA+\": 2, ..., \"D\":22 }\n",
        "moody_rating_mapping = { \"Aaa\": 1, \"Aa1\": 2, ..., \"C\": 21 }"
      ],
      "metadata": {
        "id": "oultpalfZK5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorizing Numeric Ratings\n",
        "The script categorizes numeric ratings into 'A and above', 'BBB', or 'Junk', facilitating easier qualitative analysis."
      ],
      "metadata": {
        "id": "bfD4147DZMpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rating_to_category(rating):\n",
        "    if pd.isna(rating):\n",
        "        return None\n",
        "    if 0 <= rating <= 6:\n",
        "        return 'A and above'\n",
        "    elif 7 <= rating <= 9:\n",
        "        return 'BBB'\n",
        "    else:\n",
        "        return 'Junk'"
      ],
      "metadata": {
        "id": "-UmruEGpZPdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering and Processing Ratings\n",
        "Defines functions to filter the DataFrame for S&P and Moody's ratings, convert them using the mappings, remove duplicates, and assign categories based on the converted ratings."
      ],
      "metadata": {
        "id": "rp95nlD5ZRVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sp_rating(df):\n",
        "    rat = df[(df['rating_type'] == \"SPR\")]\n",
        "    rat[\"spr\"] = rat[\"rating\"].map(sp_rating_mapping)\n",
        "    rat['category'] = rat['spr'].apply(rating_to_category)\n",
        "    return rat.drop_duplicates(subset=['issue_id', 'rating_date'])\n",
        "\n",
        "def get_moody_rating(df):\n",
        "    rat = df[(df['rating_type'] == \"MR\")]\n",
        "    rat[\"mr\"] = rat[\"rating\"].map(moody_rating_mapping)\n",
        "    rat['category'] = rat['mr'].apply(rating_to_category)\n",
        "    return rat.drop_duplicates(subset=['issue_id', 'rating_date'])"
      ],
      "metadata": {
        "id": "DXL1hg4fZTaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging and Cleaning the Data\n",
        "Combines the processed S&P and Moody's ratings, ensures completeness, and removes non-rated or special cases, finalizing the dataset for export."
      ],
      "metadata": {
        "id": "-ntrKyv_ZUFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_moody_sp(ratsp, ratsmd):\n",
        "    df = pd.concat([ratsp, ratsmd], axis=0)\n",
        "    df['spr'] = df['spr'].fillna(df['mr'])\n",
        "    return df.drop_duplicates(subset=['issue_id', 'rating_date'])"
      ],
      "metadata": {
        "id": "5uHTi8eAZXJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution and Data Saving\n",
        "The main execution block connects to WRDS, downloads rating data, merges with issue IDs, processes ratings for both S&P and Moody's, combines them, filters out non-relevant ratings, and saves the categorized ratings."
      ],
      "metadata": {
        "id": "d3ZOjl0DZZer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    db = wrds.Connection(wrds_username=WRDS_USERNAME)\n",
        "    rat_raw = db.raw_sql(\"\"\"SELECT issue_id, rating_type, rating_date, rating FROM fisd.fisd_ratings\"\"\")\n",
        "    id = db.raw_sql(\"\"\"SELECT complete_cusip, issue_id, offering_date FROM fisd.fisd_mergedissue\"\"\")\n",
        "\n",
        "    rat = pd.merge(rat_raw, id, how='inner', on='issue_id')\n",
        "    ratsp = get_sp_rating(rat[rat['rating_type'] == \"SPR\"])\n",
        "    ratsmd = get_moody_rating(rat[rat['rating_type'] == \"MR\"])\n",
        "\n",
        "    rating = concat_moody_sp(ratsp, ratsmd)\n",
        "    rating = rating[~rating['rating'].isin([\"NR\", 'NR/NR', 'SUSP', 'P-1', '0', 'NAV'])]\n",
        "\n",
        "    rating.sort_values(['complete_cusip', 'rating_date'], inplace=True)\n",
        "    rating.to_csv(Path(DATA_DIR) / \"pulled\" / 'rating.csv', index=False)"
      ],
      "metadata": {
        "id": "TZKq4abFZbYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script successfully categorizes bond ratings into three broad quality categories based on Moody's and S&P's ratings, providing a simplified but insightful view of bond credit quality for further analysis."
      ],
      "metadata": {
        "id": "yORmYLa2ZeHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Downloading and Processing Bond Market Data\n",
        "This script automates the process of downloading a compressed dataset of bond market transactions for December 2023 from a public source, extracting the contents, and preparing the data for analysis. The data is then loaded into a pandas DataFrame, with some initial cleaning applied to standardize column names and formats."
      ],
      "metadata": {
        "id": "RhaTwQPvHAUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the Dataset\n",
        "The script uses the requests library to fetch a compressed (.zip) file containing the bond market data from a specified URL."
      ],
      "metadata": {
        "id": "GrTDzYemHDQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "file_url = 'https://openbondassetpricing.com/wp-content/uploads/2023/12/BondDailyPublicDec2023.csv.zip'\n",
        "response = requests.get(file_url)\n",
        "\n",
        "with open('BondDailyPublicDec2023.csv.zip', 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "print('Download completed!')"
      ],
      "metadata": {
        "id": "RIkoVN6YHMLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the script sends a GET request to the specified URL to download the file. It then saves the file locally, indicating completion with a print statement."
      ],
      "metadata": {
        "id": "FnoyerY5HN5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting the Dataset\n",
        "After downloading, the script uses the zipfile library to extract the contents of the .zip file to a specified directory."
      ],
      "metadata": {
        "id": "U8KPtr6HHPpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('BondDailyPublicDec2023.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data')\n",
        "\n",
        "print('Extraction completed! The file is now under folder data')"
      ],
      "metadata": {
        "id": "aAC91zRWHRoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This segment opens the downloaded .zip file in read mode and extracts all files within it to the 'data' directory. It confirms completion by printing a message."
      ],
      "metadata": {
        "id": "0bYg3r8eHUQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and Initial Data Cleaning\n",
        "Finally, the script loads the extracted CSV file into a pandas DataFrame, performs some initial cleaning steps such as converting column names to lowercase and parsing dates, and removes any unnecessary columns."
      ],
      "metadata": {
        "id": "8fnf4wDKHX-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(r'data/BondDailyPublic.csv.gzip', compression='gzip')\n",
        "data.columns = data.columns.str.lower()\n",
        "data['trd_exctn_dt'] = pd.to_datetime(data['trd_exctn_dt'])\n",
        "\n",
        "if 'unnamed: 0' in data.columns:\n",
        "    data.drop(columns=['unnamed: 0'], inplace=True)"
      ],
      "metadata": {
        "id": "tvOLl_GDHZsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is read into a DataFrame with column names converted to lowercase for consistency. The script also converts the trading execution date column to datetime format for easier manipulation. If there are any unnamed columns (often artifacts from the CSV format or index columns), these are removed to clean the dataset further."
      ],
      "metadata": {
        "id": "-ad7i-DrHbc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Enhancing Bond Market Data Analysis: Processing for Daily Returns and Credit Spread\n",
        "Overview\n",
        "This Python script is a crucial component of a larger workflow aimed at analyzing the bond market. Specifically, it:\n",
        "\n",
        "1. Accepts bond market data generated by a previous script (load_return_cs.py).\n",
        "2. Cleans and filters the data based on specific conditions derived from academic research.\n",
        "3. Calculates daily returns and credit spreads for corporate bonds, outputting the refined data for further analysis."
      ],
      "metadata": {
        "id": "2N0uuyprHcVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initial Data Preparation\n",
        "The script starts by importing necessary libraries and setting up environment configurations, including data directories and warning suppression."
      ],
      "metadata": {
        "id": "d-uW6gNLJI59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "from pandas.tseries.offsets import CustomBusinessDay\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "import datetime as dt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import config\n",
        "from pathlib import Path\n",
        "\n",
        "OUTPUT_DIR = Path(config.OUTPUT_DIR)\n",
        "DATA_DIR = Path(config.DATA_DIR)\n",
        "WRDS_USERNAME = config.WRDS_USERNAME\n",
        "START_DATE = config.START_DATE\n",
        "END_DATE = config.END_DATE"
      ],
      "metadata": {
        "id": "FHPmlCgNJK2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Extracting Price and Credit Spread Information\n",
        "The first function, extract_price_cs, processes the raw data by standardizing column names, converting dates, and extracting essential information for analysis."
      ],
      "metadata": {
        "id": "MDX8BaMFJMcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_price_cs(data):\n",
        "    data.columns = data.columns.str.lower()\n",
        "    data['trd_exctn_dt'] = pd.to_datetime(data['trd_exctn_dt'])\n",
        "    if 'unnamed: 0' in data.columns:\n",
        "        data.drop(columns=['unnamed: 0'], inplace=True)\n",
        "\n",
        "    data.sort_values(by=['cusip_id', 'trd_exctn_dt'], inplace=True)\n",
        "    df_return_cs = data[['cusip_id','trd_exctn_dt','prclean','cs_dur']]\n",
        "\n",
        "    return df_return_cs"
      ],
      "metadata": {
        "id": "dGspXcK5JOQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Filtering Based on Business Days\n",
        "Next, filter_less_than_five_busn_days removes any trades with more than a five-day gap, ensuring analysis only includes bonds with regular trading activity."
      ],
      "metadata": {
        "id": "4C-yGaCoJPt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_less_than_five_busn_days(data):\n",
        "    data['days_since_last_trade'] = data.groupby('cusip_id')['trd_exctn_dt'].diff().dt.days.fillna(0).astype(int)\n",
        "    data['business_days_since_last_trade'] = data.apply(calculate_business_days, axis=1)\n",
        "    data = data[data['business_days_since_last_trade'] <= 5]\n",
        "    return data"
      ],
      "metadata": {
        "id": "A1YkzTXNJSne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Monthly Trade Volume Filtering\n",
        "The script further filters out bonds with fewer than five trades per month to focus on more liquid assets."
      ],
      "metadata": {
        "id": "vCCMnpTGJUHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_less_than_five_trades_per_months(data):\n",
        "    data['year_month'] = data['trd_exctn_dt'].dt.to_period('M')\n",
        "    eligible_bonds = data.groupby(['cusip_id', 'year_month']).size().reset_index(name='monthly_trades')\n",
        "    data = data.merge(eligible_bonds[['cusip_id', 'year_month']], on=['cusip_id', 'year_month'], how='inner')\n",
        "    return data"
      ],
      "metadata": {
        "id": "j-m9ojw8JWVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Calculating Daily Returns and Handling Reversals\n",
        "The final processing step involves calculating daily returns, removing significant reversals, and converting returns to basis points."
      ],
      "metadata": {
        "id": "GkvxVC83Jaf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_daily_returns_remove_large_reversals(data):\n",
        "    data['daily_return'] = data.groupby('cusip_id')['prclean'].pct_change()\n",
        "    data = data[~((abs(data['daily_return']) >= 0.2) & (data['daily_return'] * data['previous_return'] < 0))]\n",
        "    data = data[abs(data['daily_return']) <= 0.2]\n",
        "    data['daily_return'] = data['daily_return'] * 10000\n",
        "    data.rename(columns={'daily_return': 'daily_return_bps', 'cs_dur': 'cs_dur_bps'}, inplace=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "qqj--UGxJeFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Execution\n",
        "The script reads the initial dataset, applies the defined processing functions sequentially, and saves the cleaned and enhanced dataset to a CSV file for further analysis."
      ],
      "metadata": {
        "id": "VIR5rZjLJevw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    raw_price_cs = pd.read_csv(Path(DATA_DIR) / \"pulled\" /'BondDailyPublic.csv.gzip', compression='gzip')\n",
        "    df = extract_price_cs(raw_price_cs)\n",
        "    df_filter_less_five = filter_less_than_five_busn_days(df)\n",
        "    df_less_five_trade = filter_less_than_five_trades_per_months(df_filter_less_five)\n",
        "    df_final = calc_daily_returns_remove_large_reversals(df_less_five_trade)\n",
        "    df_final.to_csv(Path(DATA_DIR) / \"pulled\" / 'daily_return_cs.csv', index=False)"
      ],
      "metadata": {
        "id": "4LWyD8qbJgiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This structured approach ensures that the bond market data is cleaned, filtered, and enhanced accurately and efficiently, making it ready for in-depth financial analysis or modeling."
      ],
      "metadata": {
        "id": "rSppowYfJi1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Integrating and Analyzing Bond Market Data with Ratings\n",
        "This Python script performs advanced processing on bond market data, integrating various sources including trading data, bid-ask spread, returns, and bond ratings. The goal is to prepare a comprehensive dataset for in-depth analysis, specifically focusing on calculating daily returns, credit spreads, and correlating these with bond ratings. The final output includes a LaTeX table summarizing key statistics across different market periods."
      ],
      "metadata": {
        "id": "vhPeMJryLSN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initial Setup and Data Preparation\n",
        "The script begins by importing necessary libraries and configuring environment settings, including output and data directories."
      ],
      "metadata": {
        "id": "CReJgPGvLXM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "from pandas.tseries.offsets import CustomBusinessDay\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "import datetime as dt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import config\n",
        "from pathlib import Path\n",
        "\n",
        "OUTPUT_DIR = Path(config.OUTPUT_DIR)\n",
        "DATA_DIR = Path(config.DATA_DIR)\n",
        "WRDS_USERNAME = config.WRDS_USERNAME\n",
        "START_DATE = config.START_DATE\n",
        "END_DATE = config.END_DATE"
      ],
      "metadata": {
        "id": "NhxlkdaMLbtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Processing Rating Data\n",
        "process_rating_data prepares the raw rating data for merging by renaming columns and sorting."
      ],
      "metadata": {
        "id": "JJ5LTuhdLdq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_rating_data(df):\n",
        "    df = df[['complete_cusip','rating_date', 'rating','category']].sort_values(by=['complete_cusip','rating_date']).reset_index(drop=True)\n",
        "    df = df.rename(columns = {'complete_cusip':'cusip_id', 'rating_date':'date'})\n",
        "    return df"
      ],
      "metadata": {
        "id": "IYWoEq-oLfvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Merging DataFrames\n",
        "merge_df combines two datasets, such as bid-ask spread and returns, matching by bond ID and date."
      ],
      "metadata": {
        "id": "PFfrplfSLhpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_df(df1, df2):\n",
        "    try:\n",
        "        df1 = df1.rename(columns = {'trd_exctn_dt':'date'})\n",
        "        df2 = df2.rename(columns = {'trd_exctn_dt':'date'})\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    res_df = pd.merge(df1,df2,on=['cusip_id','date'],how='inner')\n",
        "    return res_df"
      ],
      "metadata": {
        "id": "abLBZtyTLjOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Integrating Ratings with Market Data\n",
        "merge_rating aligns processed market data with rating data, ensuring ratings are forward-filled to match trade dates."
      ],
      "metadata": {
        "id": "5nuRx1jxLk5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_rating(data, rating):\n",
        "    data['source'] = 'A'\n",
        "    rating['source'] = 'R'\n",
        "\n",
        "    df = pd.concat([data,rating],axis=0).sort_values(by=['cusip_id','date','source'])\n",
        "    df_filled = df.groupby('cusip_id').apply(lambda group: group.ffill()).reset_index(drop=True)\n",
        "    df_filled = df_filled[df_filled['source']=='A'][df_filled['category'].notna()].reset_index(drop=True).drop(columns = ['source','rating'])\n",
        "\n",
        "    return df_filled"
      ],
      "metadata": {
        "id": "LIFUJupILnN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Deriving Summary Statistics Table\n",
        "derive_table calculates mean values for spread, bias, daily returns, and credit spread duration across various market periods, producing a LaTeX table for publication or further analysis."
      ],
      "metadata": {
        "id": "-PNMX5ewLouT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derive_table(res_df):\n",
        "    df = res_df.copy()\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    # Subsample definitions\n",
        "    subsamples = {...}\n",
        "\n",
        "    mean_values = {}\n",
        "    for subsample, (start_date, end_date) in subsamples.items():\n",
        "        ...\n",
        "    for k,v in mean_values.items():\n",
        "        ...\n",
        "    res_df['variables'] = res_df.index\n",
        "    res_df = res_df.set_index(['category','variables'])\n",
        "\n",
        "    return res_df"
      ],
      "metadata": {
        "id": "nu9rRWBeLsFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main Execution\n",
        "The script reads in trading data, spread-bias data, and ratings, processes each, and merges them to create a unified dataset. It then computes summary statistics and formats them into a LaTeX table, saved for easy inclusion in documents."
      ],
      "metadata": {
        "id": "1TukMGoDLt86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Loading data\n",
        "    spreadbias, ret, rating = ...\n",
        "    # Processing and merging data\n",
        "    all_df = merge_df(spreadbias, ret_cs)\n",
        "    all_df = merge_rating(all_df, rating)\n",
        "    # Deriving the summary table and saving as LaTeX\n",
        "    res_df = derive_table(all_df)\n",
        "    with open(Path(OUTPUT_DIR) / 'derived_table.tex', \"w\") as text_file:\n",
        "        text_file.write(res_df.to_latex(float_format=lambda x: '{:.3f}'.format(x)))"
      ],
      "metadata": {
        "id": "q16UB8P0Lva2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script showcases a comprehensive approach to bond market data analysis, integrating diverse data sources to derive insightful metrics and summarizing the findings in a format suitable for academic publication."
      ],
      "metadata": {
        "id": "_fsH4yG1Lxu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this comprehensive analysis workflow, we utilized bond market trading data, bid-ask spreads, daily returns, credit spreads, and bond ratings sourced from various reliable databases. Our objective was to prepare a detailed and cleaned dataset that not only highlights the intricate dynamics of the bond market but also associates these financial metrics with the credit quality indicated by bond ratings.\n",
        "\n",
        "Starting from the initial setup that involved importing necessary libraries and setting up configurations, we meticulously processed the raw data. This included standardizing column names, ensuring consistent date formats, and sorting data for efficient processing. We integrated key pieces of information such as bond IDs, trading dates, prices, and credit spreads into a cohesive dataset.\n",
        "\n",
        "Through a series of functions, we merged trading data with bid-ask spreads and returns, while another critical step involved integrating the processed market data with bond ratings. This integration allowed us to forward-fill ratings to match the trading dates, ensuring each bond's financial metrics were accurately paired with its credit rating.\n",
        "\n",
        "The workflow was designed to filter out irrelevant or insufficiently traded bonds, focusing on those with a minimum trading frequency. This ensured the reliability and relevance of our analysis. Additionally, we calculated daily returns, adjusted for extreme values to avoid skewing the analysis, and converted these returns and credit spreads into basis points for a standardized comparison.\n",
        "\n",
        "Ultimately, we derived a comprehensive table summarizing key financial metrics across different market periods, such as the full sample period, pre-crisis, crisis, post-crisis, and more. This table, formatted in LaTeX for easy inclusion in academic publications, provides a clear overview of the bond market's behavior over time, segmented by credit quality categories.\n",
        "\n",
        "In conclusion, by leveraging a rich dataset encompassing various aspects of the bond market and employing a detailed processing and analysis pipeline, we achieved our goal of elucidating the complex interplay between bond market dynamics and credit ratings. This not only aids in academic research but also supports investors and analysts in making informed decisions based on comprehensive market insights."
      ],
      "metadata": {
        "id": "JZp2b00rOWb8"
      }
    }
  ]
}