{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project Guidance: Noisy Prices and Return-based Anomalies in Corporate Bonds\n",
        "- Task: Replicate Table 1\n",
        "- Data used: FINRA TRACE\n",
        "- Available code and data: https://openbondassetpricing.com/code/"
      ],
      "metadata": {
        "id": "x41myQTIR90w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Bond Data Processing for Price, Volume, and Illiquidity Analysis\n",
        "This Python script is designed to streamline the process of extracting, cleaning, and analyzing bond data from the WRDS TRACE database. The script focuses on three key areas of bond market analysis: price, volume, and illiquidity metrics. By leveraging data directly from WRDS, the script ensures access to comprehensive and accurate bond trading information, enabling a detailed examination of market dynamics."
      ],
      "metadata": {
        "id": "TF1aEEJJTJfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization and Environment Setup\n",
        "This section imports necessary libraries and sets up pandas options to suppress warnings, defining paths for data output and storage."
      ],
      "metadata": {
        "id": "ad6vZuLtUVud"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKunI3EGRj-0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wrds\n",
        "from itertools import chain\n",
        "import datetime as dt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import config\n",
        "from pathlib import Path\n",
        "OUTPUT_DIR = Path(config.OUTPUT_DIR)\n",
        "DATA_DIR = Path(config.DATA_DIR)\n",
        "WRDS_USERNAME = config.WRDS_USERNAME"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries like pandas and numpy are imported for data manipulation, while wrds is used for database access. Warning suppression and directory paths are set up for efficient data handling."
      ],
      "metadata": {
        "id": "NWDwQsMTUd9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting to WRDS Database\n",
        "This snippet establishes a connection to the WRDS database using the username provided in the configuration file."
      ],
      "metadata": {
        "id": "tcpJoXhSUm3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = wrds.Connection(wrds_username=WRDS_USERNAME)"
      ],
      "metadata": {
        "id": "dzqywK_iUpXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The connection to the WRDS database is essential for querying and downloading bond data directly from their platform."
      ],
      "metadata": {
        "id": "UiVm0nswUqyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading and Processing Mergent Files\n",
        "Queries the WRDS database to download issuer and issue data, merging them based on issuer_id."
      ],
      "metadata": {
        "id": "8-uJ5Zt2UtW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fisd_issuer = db.raw_sql(\"\"\"SELECT issuer_id,country_domicile FROM fisd.fisd_mergedissuer\"\"\")\n",
        "fisd_issue = db.raw_sql(\"\"\"SELECT ... FROM fisd.fisd_mergedissue\"\"\")\n",
        "fisd = pd.merge(fisd_issue, fisd_issuer, on=['issuer_id'], how=\"left\")"
      ],
      "metadata": {
        "id": "9kw3wL4uUwr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step retrieves and combines issuer and issue data from WRDS, creating a comprehensive dataset for bond analysis."
      ],
      "metadata": {
        "id": "qe0DVsmnUyQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering Bond Data\n",
        "Applies filters to the merged dataset to retain bonds meeting specific criteria, such as being US-based and non-convertible."
      ],
      "metadata": {
        "id": "Citam4O2UzvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fisd = fisd[(fisd.country_domicile == 'USA') & (fisd.convertible == 'N')]\n",
        "...\n",
        "fisd = fisd[~fisd.dated_date.isnull()]"
      ],
      "metadata": {
        "id": "aqj3g9HbU5Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filters are used to refine the bond dataset, focusing on bonds of interest based on predefined criteria."
      ],
      "metadata": {
        "id": "x5CvMdw9U6TI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the Processed FISD Data\n",
        "The filtered dataset is saved to a specified directory for future use, ensuring the processed data is accessible for analysis or further processing."
      ],
      "metadata": {
        "id": "gdgUALGnU8Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = './' / Path(DATA_DIR) / \"pulled\" / \"fisd.csv\"\n",
        "fisd.to_csv(path, index=False)"
      ],
      "metadata": {
        "id": "6sP63NvCU_nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The processed and filtered bond data is stored in a CSV file, making it readily available for subsequent analysis or steps in the processing pipeline."
      ],
      "metadata": {
        "id": "312tsQzRVBWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing and Cleaning Data for Analysis\n",
        "Additional preparation and cleaning steps are undertaken, including ensuring unique identifiers for bonds and dividing the dataset into manageable chunks for processing."
      ],
      "metadata": {
        "id": "28M7NGZ5VC3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CUSIP_Sample = list(fisd['complete_cusip'].unique())\n",
        "cusip_chunks = list(divide_chunks(CUSIP_Sample, 500))"
      ],
      "metadata": {
        "id": "aFYvlGK6VIIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section involves further preparation of the bond data, including deduplication and segmentation into smaller chunks for efficient processing."
      ],
      "metadata": {
        "id": "ng1FItpZVKvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iterative Data Cleaning and Analysis for Prices, Volume, and Illiquidity\n",
        "Processes each data chunk to calculate daily prices, volumes, and illiquidity metrics."
      ],
      "metadata": {
        "id": "2xKz008FVMB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, len(cusip_chunks)):\n",
        "    trace = db.raw_sql('SELECT ... FROM trace.trace_enhanced WHERE cusip_id in %(cusip_id)s', params=parm)\n",
        "    ...\n",
        "    # Calculate Equal-Weighted Prices\n",
        "    prc_EW = trace.groupby(['cusip_id', 'trd_exctn_dt'])['rptd_pr'].mean().to_frame(name='prc_ew')\n",
        "    # Calculate Volume-Weighted Prices\n",
        "    trace['dollar_vol'] = trace['entrd_vol_qt'] * trace['rptd_pr'] / 100\n",
        "    prc_VW = trace.groupby(['cusip_id', 'trd_exctn_dt']).apply(lambda x: np.sum(x['rptd_pr'] * (x['entrd_vol_qt'] / x['entrd_vol_qt'].sum()))).to_frame(name='prc_vw')\n",
        "    # Calculate Volumes\n",
        "    VolumesAll = trace.groupby(['cusip_id', 'trd_exctn_dt'])['entrd_vol_qt'].sum().to_frame(name='volume')\n",
        "    # Calculate Illiquidity Measures\n",
        "    bid_ask_spread = calculate_bid_ask_spread(trace)"
      ],
      "metadata": {
        "id": "OgOKXXpEVS0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section performs comprehensive data cleaning and calculates key financial metrics, including prices (equal-weighted and volume-weighted), trading volumes, and illiquidity measures such as the bid-ask spread."
      ],
      "metadata": {
        "id": "J_t_922XXWQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving Final Analysis Results\n",
        "Stores the calculated prices, volumes, and illiquidity metrics for further analysis or reporting."
      ],
      "metadata": {
        "id": "2g9-SlBIVVlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PricesExport.to_csv('./' / Path(DATA_DIR) / \"pulled\" / 'Prices.csv.gzip', compression='gzip')\n",
        "VolumeExport.to_csv('./' / Path(DATA_DIR) / \"pulled\" / 'Volumes.csv.gzip', compression='gzip')\n",
        "IlliqExport.to_csv('./' / Path(DATA_DIR) / \"pulled\" / 'Illiq.csv.gzip', compression='gzip')"
      ],
      "metadata": {
        "id": "lh6gIb3MVY7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code saves the analysis results in compressed GZIP format, facilitating efficient storage and access. Through this comprehensive approach, the process not only provides insights into daily bond prices and trading volumes but also evaluates market liquidity, offering a deep dive into the dynamics of the bond market."
      ],
      "metadata": {
        "id": "hqW3jV_5Vax8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Converting WRDS Bond Ratings to Categorical Scores\n",
        "This Python script automates fetching bond ratings from the WRDS (Wharton Research Data Services) database, specifically targeting Moody's and Standard & Poor's (S&P) ratings. Instead of converting these ratings into numeric scores, it categorizes them into three broad quality categories: 'A and above', 'BBB', or 'Junk'. The script then cleans and saves the processed data for subsequent analysis."
      ],
      "metadata": {
        "id": "aGOfgoCUXjyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization and Setup\n",
        "The script begins by importing necessary libraries, suppressing warnings, and setting up directories for output."
      ],
      "metadata": {
        "id": "Ix-OKUt7ZDjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from dateutil.relativedelta import *\n",
        "from pandas.tseries.offsets import *\n",
        "import datetime as datetime\n",
        "import wrds\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import config\n",
        "from pathlib import Path\n",
        "OUTPUT_DIR = Path(config.OUTPUT_DIR)\n",
        "DATA_DIR = Path(config.DATA_DIR)\n",
        "WRDS_USERNAME = config.WRDS_USERNAME"
      ],
      "metadata": {
        "id": "g0HFU16lZGCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ratings Conversion Mappings\n",
        "Although mappings for S&P and Moody's ratings to numeric values are defined, the primary goal is to categorize these ratings into qualitative buckets."
      ],
      "metadata": {
        "id": "O3BlIoNqZHiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp_rating_mapping = { \"AAA\": 1, \"AA+\": 2, ..., \"D\":22 }\n",
        "moody_rating_mapping = { \"Aaa\": 1, \"Aa1\": 2, ..., \"C\": 21 }"
      ],
      "metadata": {
        "id": "oultpalfZK5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorizing Numeric Ratings\n",
        "The script categorizes numeric ratings into 'A and above', 'BBB', or 'Junk', facilitating easier qualitative analysis."
      ],
      "metadata": {
        "id": "bfD4147DZMpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rating_to_category(rating):\n",
        "    if pd.isna(rating):\n",
        "        return None\n",
        "    if 0 <= rating <= 6:\n",
        "        return 'A and above'\n",
        "    elif 7 <= rating <= 9:\n",
        "        return 'BBB'\n",
        "    else:\n",
        "        return 'Junk'"
      ],
      "metadata": {
        "id": "-UmruEGpZPdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filtering and Processing Ratings\n",
        "Defines functions to filter the DataFrame for S&P and Moody's ratings, convert them using the mappings, remove duplicates, and assign categories based on the converted ratings."
      ],
      "metadata": {
        "id": "rp95nlD5ZRVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sp_rating(df):\n",
        "    rat = df[(df['rating_type'] == \"SPR\")]\n",
        "    rat[\"spr\"] = rat[\"rating\"].map(sp_rating_mapping)\n",
        "    rat['category'] = rat['spr'].apply(rating_to_category)\n",
        "    return rat.drop_duplicates(subset=['issue_id', 'rating_date'])\n",
        "\n",
        "def get_moody_rating(df):\n",
        "    rat = df[(df['rating_type'] == \"MR\")]\n",
        "    rat[\"mr\"] = rat[\"rating\"].map(moody_rating_mapping)\n",
        "    rat['category'] = rat['mr'].apply(rating_to_category)\n",
        "    return rat.drop_duplicates(subset=['issue_id', 'rating_date'])"
      ],
      "metadata": {
        "id": "DXL1hg4fZTaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging and Cleaning the Data\n",
        "Combines the processed S&P and Moody's ratings, ensures completeness, and removes non-rated or special cases, finalizing the dataset for export."
      ],
      "metadata": {
        "id": "-ntrKyv_ZUFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_moody_sp(ratsp, ratsmd):\n",
        "    df = pd.concat([ratsp, ratsmd], axis=0)\n",
        "    df['spr'] = df['spr'].fillna(df['mr'])\n",
        "    return df.drop_duplicates(subset=['issue_id', 'rating_date'])"
      ],
      "metadata": {
        "id": "5uHTi8eAZXJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution and Data Saving\n",
        "The main execution block connects to WRDS, downloads rating data, merges with issue IDs, processes ratings for both S&P and Moody's, combines them, filters out non-relevant ratings, and saves the categorized ratings."
      ],
      "metadata": {
        "id": "d3ZOjl0DZZer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    db = wrds.Connection(wrds_username=WRDS_USERNAME)\n",
        "    rat_raw = db.raw_sql(\"\"\"SELECT issue_id, rating_type, rating_date, rating FROM fisd.fisd_ratings\"\"\")\n",
        "    id = db.raw_sql(\"\"\"SELECT complete_cusip, issue_id, offering_date FROM fisd.fisd_mergedissue\"\"\")\n",
        "\n",
        "    rat = pd.merge(rat_raw, id, how='inner', on='issue_id')\n",
        "    ratsp = get_sp_rating(rat[rat['rating_type'] == \"SPR\"])\n",
        "    ratsmd = get_moody_rating(rat[rat['rating_type'] == \"MR\"])\n",
        "\n",
        "    rating = concat_moody_sp(ratsp, ratsmd)\n",
        "    rating = rating[~rating['rating'].isin([\"NR\", 'NR/NR', 'SUSP', 'P-1', '0', 'NAV'])]\n",
        "\n",
        "    rating.sort_values(['complete_cusip', 'rating_date'], inplace=True)\n",
        "    rating.to_csv(Path(DATA_DIR) / \"pulled\" / 'rating.csv', index=False)"
      ],
      "metadata": {
        "id": "TZKq4abFZbYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script successfully categorizes bond ratings into three broad quality categories based on Moody's and S&P's ratings, providing a simplified but insightful view of bond credit quality for further analysis."
      ],
      "metadata": {
        "id": "yORmYLa2ZeHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Downloading and Processing Bond Market Data\n",
        "This Python script is designed to automate the process of downloading, extracting, and managing the latest financial data on bonds from the website https://openbondassetpricing.com. Specifically, it fetches daily TRACE panel data that includes various critical financial metrics such as clean and invoice (dirty) prices, accrued interest, daily bond yields, bond credit spreads, duration, and convexity."
      ],
      "metadata": {
        "id": "RhaTwQPvHAUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing Necessary Libraries\n",
        "The script starts by importing necessary Python libraries and modules to facilitate web requests, data manipulation, file management, and to suppress warnings."
      ],
      "metadata": {
        "id": "GrTDzYemHDQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import warnings\n",
        "import zipfile\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "-Ry4F0qg5EeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Configuration and Directory Setup\n",
        "It then imports configuration settings from a separate config file, which likely contains paths to directories and authentication credentials. Using the Path class from pathlib, it sets up directories for output and data storage, as well as retrieves a username for WRDS (Wharton Research Data Services) which might be used for data access authentication."
      ],
      "metadata": {
        "id": "4Aok0nYG5KVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import config\n",
        "from pathlib import Path\n",
        "OUTPUT_DIR = Path(config.OUTPUT_DIR)\n",
        "DATA_DIR = Path(config.DATA_DIR)\n",
        "WRDS_USERNAME = config.WRDS_USERNAME"
      ],
      "metadata": {
        "id": "mZTexwhS5L2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Downloading the Data File\n",
        "The script defines a URL pointing to a compressed CSV file (BondDailyPublicDec2023.csv.zip) containing the relevant bond data for December 2023. Using the requests library, it fetches this file and writes its content to the local filesystem, indicating completion of the download with a printed message."
      ],
      "metadata": {
        "id": "cwf0xxuD5PtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_url = 'https://openbondassetpricing.com/wp-content/uploads/2023/12/BondDailyPublicDec2023.csv.zip'\n",
        "response = requests.get(file_url)\n",
        "with open('BondDailyPublicDec2023.csv.zip', 'wb') as file:\n",
        "    file.write(response.content)\n",
        "print('Download completed!')"
      ],
      "metadata": {
        "id": "6HmwUL4_5R7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Extracting the File Contents\n",
        "After downloading the zip file, the script uses the zipfile library to extract its contents into a designated subdirectory within the DATA_DIR directory. It confirms completion of the extraction process by printing another message."
      ],
      "metadata": {
        "id": "_2vBxdhy5UA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('BondDailyPublicDec2023.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(Path(DATA_DIR) / \"pulled\")\n",
        "print('Extraction completed! The file is now under folder \\data\\pulled')"
      ],
      "metadata": {
        "id": "M3Ly6ZzC5XbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cleanup\n",
        "Lastly, the script checks if the downloaded zip file still exists in the local directory. If it does, the script deletes the file to clean up and frees up space, confirming the deletion with a printed message. If the file doesn't exist, it notifies the user accordingly."
      ],
      "metadata": {
        "id": "y4cAlCsL5Y_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'BondDailyPublicDec2023.csv.zip'\n",
        "if os.path.exists(path):\n",
        "    os.remove(path)\n",
        "    print(f\"File {path} has been deleted.\")\n",
        "else:\n",
        "    print(f\"The file {path} does not exist.\")"
      ],
      "metadata": {
        "id": "VTgZY-tC5bjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This entire script automates the workflow of obtaining, extracting, and managing financial data for bonds, streamlining the process for users needing the latest data for analysis or reporting."
      ],
      "metadata": {
        "id": "3rpoqfSh5d7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Calculating Bid-Ask Spread and Bias in Bond Market Data\n",
        "This Python script is designed to enhance bond market analysis by merging rating data with illiquid data (value-weighted bid and ask prices), cleaning the merged dataset, and subsequently calculating the bid-ask spread and bid-ask bias. These metrics are essential for understanding market liquidity and pricing efficiency."
      ],
      "metadata": {
        "id": "q5nK1ZEC5m1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Processing Illiquid Data\n",
        "The first step involves preparing the illiquid data for analysis, focusing on renaming columns and sorting."
      ],
      "metadata": {
        "id": "pu-uVd1b5-W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_illiquid_data(df):\n",
        "    df = df.rename(columns = {'trd_exctn_dt':'date'})\n",
        "    df = df.sort_values(by=['cusip_id','date']).reset_index(drop=True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "QF3MMufR6AQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes the raw illiquid data retrieved from load_trace.py, renames the trading date column for consistency, and sorts the data by bond ID and date for orderly processing."
      ],
      "metadata": {
        "id": "_CWSepCR6DUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Extracting Trade Information\n",
        "Next, the script processes the illiquid data further by calculating the business days between two trades and the total number of trades for each bond within a month."
      ],
      "metadata": {
        "id": "ApB_4dpU6E4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trades_info(df, start_date = START_DATE, end_date = END_DATE):\n",
        "    calendar = USFederalHolidayCalendar()\n",
        "    holidays = calendar.holidays(start_date, end_date)\n",
        "\n",
        "    df['date_lag'] = df.groupby('cusip_id')['date'].shift(1)\n",
        "    dfDC = df.dropna()\n",
        "    dfDC['n']  = np.busday_count(dfDC['date_lag'].values.astype('M8[D]'), dfDC['date'].values.astype('M8[D]'), holidays=holidays.date.tolist())\n",
        "\n",
        "    df = df.merge(dfDC[['cusip_id','date','n']], on=['cusip_id','date'], how=\"left\").dropna()\n",
        "    df['month_year'] = pd.to_datetime(df['date']).dt.to_period('M')\n",
        "    df['trade_counts'] = df.groupby(['cusip_id','month_year'])['date'].transform(\"count\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "jBZF5p_N6Gfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function enhances the dataset by adding columns to denote the business days count between two trades and the total number of trades for a bond within a month, which are crucial for liquidity analysis."
      ],
      "metadata": {
        "id": "TISBx-yq6IHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Calculating Spread and Bias\n",
        "The script then calculates the bid-ask spread and bid-ask bias for each trade, applying winsorization as suggested in academic literature to limit extreme values."
      ],
      "metadata": {
        "id": "mjQiRylc6Jem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_spread_bias(df):\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df['spread'] = (df['prc_bid'] - df['prc_ask'])/(df['prc_bid'] + df['prc_ask']) * 10000 * 2\n",
        "    df['bias'] = (((df['prc_bid'] - df['prc_ask']) / (df['prc_bid'] + df['prc_ask'])) ** 2) * 10000\n",
        "    df['winsorized_bias'] = winsorize(df['bias'], limits=[0.005, 0.005])\n",
        "    return df"
      ],
      "metadata": {
        "id": "TlI0udjk6K-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This critical function calculates the bid-ask spread and bias for each bond, with the spread reflecting the difference between the bid and ask prices and the bias indicating the squared spread, thus offering insights into market efficiency."
      ],
      "metadata": {
        "id": "aCm2GAzo6MZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main Execution\n",
        "Finally, the script merges, cleans, and processes the data to output a dataset that includes calculated spreads and biases."
      ],
      "metadata": {
        "id": "6mYNzeh46Npi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    raw_illiqs = pd.read_csv(DATA_DIR / \"pulled\" /'Illiq.csv.gzip', compression='gzip')\n",
        "    illiqs = process_illiquid_data(raw_illiqs)\n",
        "    df = get_trades_info(illiqs)\n",
        "    df_wo5 = df[df['trade_counts'] >= 5]\n",
        "    df_wo5 = df_wo5[df_wo5['n'] <= 7]\n",
        "    df_final = calc_spread_bias(df_wo5)\n",
        "    df_final.sort_values(['cusip_id', 'date'], inplace = True)\n",
        "    df_res = df_final[['cusip_id', 'date', 'spread','winsorized_bias']]\n",
        "    df_res.to_csv(Path(DATA_DIR) / \"pulled\" / 'spread_bias.csv', index=False)"
      ],
      "metadata": {
        "id": "Jt_wcgWf6PFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this workflow, the script successfully merges rating data with illiquid data, cleanses the merged dataset, and computes bid-ask spread and bias metrics, thereby fulfilling the objective of enhancing bond market analysis. These steps provide vital insights into liquidity and market dynamics, supporting more informed investment decisions and academic research."
      ],
      "metadata": {
        "id": "_jLoFaNH6QxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Enhancing Bond Market Data Analysis: Processing for Daily Returns and Credit Spread\n",
        "Overview\n",
        "This Python script is a crucial component of a larger workflow aimed at analyzing the bond market. Specifically, it:\n",
        "\n",
        "1. Accepts bond market data generated by a previous script (load_return_cs.py).\n",
        "2. Cleans and filters the data based on specific conditions derived from academic research.\n",
        "3. Calculates daily returns and credit spreads for corporate bonds, outputting the refined data for further analysis."
      ],
      "metadata": {
        "id": "2N0uuyprHcVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initial Data Preparation\n",
        "The script starts by importing necessary libraries and setting up environment configurations, including data directories and warning suppression."
      ],
      "metadata": {
        "id": "d-uW6gNLJI59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "from pandas.tseries.offsets import CustomBusinessDay\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "import datetime as dt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import config\n",
        "from pathlib import Path\n",
        "\n",
        "OUTPUT_DIR = Path(config.OUTPUT_DIR)\n",
        "DATA_DIR = Path(config.DATA_DIR)\n",
        "WRDS_USERNAME = config.WRDS_USERNAME\n",
        "START_DATE = config.START_DATE\n",
        "END_DATE = config.END_DATE"
      ],
      "metadata": {
        "id": "FHPmlCgNJK2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Extracting Price and Credit Spread Information\n",
        "The first function, extract_price_cs, processes the raw data by standardizing column names, converting dates, and extracting essential information for analysis."
      ],
      "metadata": {
        "id": "MDX8BaMFJMcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_price_cs(data):\n",
        "    data.columns = data.columns.str.lower()\n",
        "    data['trd_exctn_dt'] = pd.to_datetime(data['trd_exctn_dt'])\n",
        "    if 'unnamed: 0' in data.columns:\n",
        "        data.drop(columns=['unnamed: 0'], inplace=True)\n",
        "\n",
        "    data.sort_values(by=['cusip_id', 'trd_exctn_dt'], inplace=True)\n",
        "    df_return_cs = data[['cusip_id','trd_exctn_dt','prclean','cs_dur']]\n",
        "\n",
        "    return df_return_cs"
      ],
      "metadata": {
        "id": "dGspXcK5JOQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Filtering Based on Business Days\n",
        "Next, filter_less_than_five_busn_days removes any trades with more than a five-day gap, ensuring analysis only includes bonds with regular trading activity."
      ],
      "metadata": {
        "id": "4C-yGaCoJPt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_less_than_five_busn_days(data):\n",
        "    data['days_since_last_trade'] = data.groupby('cusip_id')['trd_exctn_dt'].diff().dt.days.fillna(0).astype(int)\n",
        "    data['business_days_since_last_trade'] = data.apply(calculate_business_days, axis=1)\n",
        "    data = data[data['business_days_since_last_trade'] <= 5]\n",
        "    return data"
      ],
      "metadata": {
        "id": "A1YkzTXNJSne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Monthly Trade Volume Filtering\n",
        "The script further filters out bonds with fewer than five trades per month to focus on more liquid assets."
      ],
      "metadata": {
        "id": "vCCMnpTGJUHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_less_than_five_trades_per_months(data):\n",
        "    data['year_month'] = data['trd_exctn_dt'].dt.to_period('M')\n",
        "    eligible_bonds = data.groupby(['cusip_id', 'year_month']).size().reset_index(name='monthly_trades')\n",
        "    data = data.merge(eligible_bonds[['cusip_id', 'year_month']], on=['cusip_id', 'year_month'], how='inner')\n",
        "    return data"
      ],
      "metadata": {
        "id": "j-m9ojw8JWVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Calculating Daily Returns and Handling Reversals\n",
        "The final processing step involves calculating daily returns, removing significant reversals, and converting returns to basis points."
      ],
      "metadata": {
        "id": "GkvxVC83Jaf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_daily_returns_remove_large_reversals(data):\n",
        "    data['daily_return'] = data.groupby('cusip_id')['prclean'].pct_change()\n",
        "    data = data[~((abs(data['daily_return']) >= 0.2) & (data['daily_return'] * data['previous_return'] < 0))]\n",
        "    data = data[abs(data['daily_return']) <= 0.2]\n",
        "    data['daily_return'] = data['daily_return'] * 10000\n",
        "    data.rename(columns={'daily_return': 'daily_return_bps', 'cs_dur': 'cs_dur_bps'}, inplace=True)\n",
        "    return data"
      ],
      "metadata": {
        "id": "qqj--UGxJeFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Execution\n",
        "The script reads the initial dataset, applies the defined processing functions sequentially, and saves the cleaned and enhanced dataset to a CSV file for further analysis."
      ],
      "metadata": {
        "id": "VIR5rZjLJevw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    raw_price_cs = pd.read_csv(Path(DATA_DIR) / \"pulled\" /'BondDailyPublic.csv.gzip', compression='gzip')\n",
        "    df = extract_price_cs(raw_price_cs)\n",
        "    df_filter_less_five = filter_less_than_five_busn_days(df)\n",
        "    df_less_five_trade = filter_less_than_five_trades_per_months(df_filter_less_five)\n",
        "    df_final = calc_daily_returns_remove_large_reversals(df_less_five_trade)\n",
        "    df_final.to_csv(Path(DATA_DIR) / \"pulled\" / 'daily_return_cs.csv', index=False)"
      ],
      "metadata": {
        "id": "4LWyD8qbJgiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This structured approach ensures that the bond market data is cleaned, filtered, and enhanced accurately and efficiently, making it ready for in-depth financial analysis or modeling."
      ],
      "metadata": {
        "id": "rSppowYfJi1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6: Integrating and Analyzing Bond Market Data with Ratings\n",
        "This Python script performs advanced processing on bond market data, integrating various sources including trading data, bid-ask spread, returns, and bond ratings. The goal is to prepare a comprehensive dataset for in-depth analysis, specifically focusing on calculating daily returns, credit spreads, and correlating these with bond ratings. The final output includes a LaTeX table summarizing key statistics across different market periods."
      ],
      "metadata": {
        "id": "vhPeMJryLSN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initial Setup and Data Preparation\n",
        "The script begins by importing necessary libraries and configuring environment settings, including output and data directories."
      ],
      "metadata": {
        "id": "CReJgPGvLXM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "from pandas.tseries.offsets import CustomBusinessDay\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "import datetime as dt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import config\n",
        "from pathlib import Path\n",
        "\n",
        "OUTPUT_DIR = Path(config.OUTPUT_DIR)\n",
        "DATA_DIR = Path(config.DATA_DIR)\n",
        "WRDS_USERNAME = config.WRDS_USERNAME\n",
        "START_DATE = config.START_DATE\n",
        "END_DATE = config.END_DATE"
      ],
      "metadata": {
        "id": "NhxlkdaMLbtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Processing Rating Data\n",
        "process_rating_data prepares the raw rating data for merging by renaming columns and sorting."
      ],
      "metadata": {
        "id": "JJ5LTuhdLdq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_rating_data(df):\n",
        "    df = df[['complete_cusip','rating_date', 'rating','category']].sort_values(by=['complete_cusip','rating_date']).reset_index(drop=True)\n",
        "    df = df.rename(columns = {'complete_cusip':'cusip_id', 'rating_date':'date'})\n",
        "    return df"
      ],
      "metadata": {
        "id": "IYWoEq-oLfvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Merging DataFrames\n",
        "merge_df combines two datasets, such as bid-ask spread and returns, matching by bond ID and date."
      ],
      "metadata": {
        "id": "PFfrplfSLhpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_df(df1, df2):\n",
        "    try:\n",
        "        df1 = df1.rename(columns = {'trd_exctn_dt':'date'})\n",
        "        df2 = df2.rename(columns = {'trd_exctn_dt':'date'})\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    res_df = pd.merge(df1,df2,on=['cusip_id','date'],how='inner')\n",
        "    return res_df"
      ],
      "metadata": {
        "id": "abLBZtyTLjOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Integrating Ratings with Market Data\n",
        "merge_rating aligns processed market data with rating data, ensuring ratings are forward-filled to match trade dates."
      ],
      "metadata": {
        "id": "5nuRx1jxLk5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_rating(data, rating):\n",
        "    data['source'] = 'A'\n",
        "    rating['source'] = 'R'\n",
        "\n",
        "    df = pd.concat([data,rating],axis=0).sort_values(by=['cusip_id','date','source'])\n",
        "    df_filled = df.groupby('cusip_id').apply(lambda group: group.ffill()).reset_index(drop=True)\n",
        "    df_filled = df_filled[df_filled['source']=='A'][df_filled['category'].notna()].reset_index(drop=True).drop(columns = ['source','rating'])\n",
        "\n",
        "    return df_filled"
      ],
      "metadata": {
        "id": "LIFUJupILnN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Deriving Summary Statistics Table\n",
        "derive_table calculates mean values for spread, bias, daily returns, and credit spread duration across various market periods, producing a LaTeX table for publication or further analysis."
      ],
      "metadata": {
        "id": "-PNMX5ewLouT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derive_table(res_df):\n",
        "    df = res_df.copy()\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    # Subsample definitions\n",
        "    subsamples = {...}\n",
        "\n",
        "    mean_values = {}\n",
        "    for subsample, (start_date, end_date) in subsamples.items():\n",
        "        ...\n",
        "    for k,v in mean_values.items():\n",
        "        ...\n",
        "    res_df['variables'] = res_df.index\n",
        "    res_df = res_df.set_index(['category','variables'])\n",
        "\n",
        "    return res_df"
      ],
      "metadata": {
        "id": "nu9rRWBeLsFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main Execution\n",
        "The script reads in trading data, spread-bias data, and ratings, processes each, and merges them to create a unified dataset. It then computes summary statistics and formats them into a LaTeX table, saved for easy inclusion in documents."
      ],
      "metadata": {
        "id": "1TukMGoDLt86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Loading data\n",
        "    spreadbias, ret, rating = ...\n",
        "    # Processing and merging data\n",
        "    all_df = merge_df(spreadbias, ret_cs)\n",
        "    all_df = merge_rating(all_df, rating)\n",
        "    # Deriving the summary table and saving as LaTeX\n",
        "    res_df = derive_table(all_df)\n",
        "    with open(Path(OUTPUT_DIR) / 'derived_table.tex', \"w\") as text_file:\n",
        "        text_file.write(res_df.to_latex(float_format=lambda x: '{:.3f}'.format(x)))"
      ],
      "metadata": {
        "id": "q16UB8P0Lva2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script showcases a comprehensive approach to bond market data analysis, integrating diverse data sources to derive insightful metrics and summarizing the findings in a format suitable for academic publication."
      ],
      "metadata": {
        "id": "_fsH4yG1Lxu-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this comprehensive analysis workflow, we utilized bond market trading data, bid-ask spreads, daily returns, credit spreads, and bond ratings sourced from various reliable databases. Our objective was to prepare a detailed and cleaned dataset that not only highlights the intricate dynamics of the bond market but also associates these financial metrics with the credit quality indicated by bond ratings.\n",
        "\n",
        "Starting from the initial setup that involved importing necessary libraries and setting up configurations, we meticulously processed the raw data. This included standardizing column names, ensuring consistent date formats, and sorting data for efficient processing. We integrated key pieces of information such as bond IDs, trading dates, prices, and credit spreads into a cohesive dataset.\n",
        "\n",
        "Through a series of functions, we merged trading data with bid-ask spreads and returns, while another critical step involved integrating the processed market data with bond ratings. This integration allowed us to forward-fill ratings to match the trading dates, ensuring each bond's financial metrics were accurately paired with its credit rating.\n",
        "\n",
        "The workflow was designed to filter out irrelevant or insufficiently traded bonds, focusing on those with a minimum trading frequency. This ensured the reliability and relevance of our analysis. Additionally, we calculated daily returns, adjusted for extreme values to avoid skewing the analysis, and converted these returns and credit spreads into basis points for a standardized comparison.\n",
        "\n",
        "Ultimately, we derived a comprehensive table summarizing key financial metrics across different market periods, such as the full sample period, pre-crisis, crisis, post-crisis, and more. This table, formatted in LaTeX for easy inclusion in academic publications, provides a clear overview of the bond market's behavior over time, segmented by credit quality categories.\n",
        "\n",
        "In conclusion, by leveraging a rich dataset encompassing various aspects of the bond market and employing a detailed processing and analysis pipeline, we achieved our goal of elucidating the complex interplay between bond market dynamics and credit ratings. This not only aids in academic research but also supports investors and analysts in making informed decisions based on comprehensive market insights."
      ],
      "metadata": {
        "id": "JZp2b00rOWb8"
      }
    }
  ]
}
