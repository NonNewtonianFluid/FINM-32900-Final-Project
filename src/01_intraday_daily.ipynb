{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "import wrds\n",
    "from itertools import chain\n",
    "import datetime as dt\n",
    "import zipfile\n",
    "import csv\n",
    "import gzip\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import config\n",
    "from pathlib import Path\n",
    "OUTPUT_DIR = Path(config.OUTPUT_DIR)\n",
    "DATA_DIR = Path(config.DATA_DIR)\n",
    "WRDS_USERNAME = config.WRDS_USERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to WRDS\n",
    "db = wrds.Connection(wrds_username=WRDS_USERNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download Mergent File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisd_issuer = db.raw_sql(\"\"\"SELECT issuer_id,country_domicile                 \n",
    "                  FROM fisd.fisd_mergedissuer \n",
    "                  \"\"\")\n",
    "fisd_issuer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisd_issue = db.raw_sql(\"\"\"SELECT complete_cusip, issue_id,\n",
    "                  issuer_id, foreign_currency,\n",
    "                  coupon_type,coupon,convertible,\n",
    "                  asset_backed,rule_144a,\n",
    "                  bond_type,private_placement,\n",
    "                  interest_frequency,dated_date,\n",
    "                  day_count_basis,offering_date,\n",
    "                  offering_amt, maturity, principal_amt                \n",
    "                  FROM fisd.fisd_mergedissue  \n",
    "                  \"\"\")\n",
    "fisd_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisd = pd.merge(fisd_issue, fisd_issuer, on = ['issuer_id'], how = \"left\") \n",
    "fisd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply BBW Bond Filters\n",
    "#1: Discard all non-US Bonds (i) in BBW\n",
    "fisd = fisd[(fisd.country_domicile == 'USA')]\n",
    "\n",
    "#2.1: US FX\n",
    "fisd = fisd[(fisd.foreign_currency == 'N')]\n",
    "\n",
    "#3: Must have a fixed coupon\n",
    "fisd = fisd[(fisd.coupon_type != 'V')]\n",
    "\n",
    "#4: Discard ALL convertible bonds\n",
    "fisd = fisd[(fisd.convertible == 'N')]\n",
    "\n",
    "#5: Discard all asset-backed bonds\n",
    "fisd = fisd[(fisd.asset_backed == 'N')]\n",
    "\n",
    "#6: Discard all bonds under Rule 144A\n",
    "fisd = fisd[(fisd.rule_144a == 'N')]\n",
    "\n",
    "#7: Remove Agency bonds, Muni Bonds, Government Bonds, \n",
    "mask_corp = ((fisd.bond_type != 'TXMU')&  (fisd.bond_type != 'CCOV') &  (fisd.bond_type != 'CPAS')\\\n",
    "            &  (fisd.bond_type != 'MBS') &  (fisd.bond_type != 'FGOV')\\\n",
    "            &  (fisd.bond_type != 'USTC')   &  (fisd.bond_type != 'USBD')\\\n",
    "            &  (fisd.bond_type != 'USNT')  &  (fisd.bond_type != 'USSP')\\\n",
    "            &  (fisd.bond_type != 'USSI') &  (fisd.bond_type != 'FGS')\\\n",
    "            &  (fisd.bond_type != 'USBL') &  (fisd.bond_type != 'ABS')\\\n",
    "            &  (fisd.bond_type != 'O30Y')\\\n",
    "            &  (fisd.bond_type != 'O10Y') &  (fisd.bond_type != 'O3Y')\\\n",
    "            &  (fisd.bond_type != 'O5Y') &  (fisd.bond_type != 'O4W')\\\n",
    "            &  (fisd.bond_type != 'CCUR') &  (fisd.bond_type != 'O13W')\\\n",
    "            &  (fisd.bond_type != 'O52W')\\\n",
    "            &  (fisd.bond_type != 'O26W')\\\n",
    "            # Remove all Agency backed / Agency bonds #\n",
    "            &  (fisd.bond_type != 'ADEB')\\\n",
    "            &  (fisd.bond_type != 'AMTN')\\\n",
    "            &  (fisd.bond_type != 'ASPZ')\\\n",
    "            &  (fisd.bond_type != 'EMTN')\\\n",
    "            &  (fisd.bond_type != 'ADNT')\\\n",
    "            &  (fisd.bond_type != 'ARNT'))\n",
    "fisd = fisd[(mask_corp)]\n",
    "\n",
    "#8: No Private Placement\n",
    "fisd = fisd[(fisd.private_placement == 'N')]\n",
    "\n",
    "#9: Remove floating-rate, bi-monthly and unclassified coupons\n",
    "fisd = fisd[(fisd.interest_frequency != \"-1\") ]   # Unclassified by Mergent\n",
    "fisd = fisd[(fisd.interest_frequency != \"13\") ]   # Variable Coupon (V)\n",
    "fisd = fisd[(fisd.interest_frequency != \"14\") ]   # Bi-Monthly Coupon\n",
    "fisd = fisd[(fisd.interest_frequency != \"16\") ]   # Unclassified by Mergent\n",
    "fisd = fisd[(fisd.interest_frequency != \"15\") ]   # Unclassified by Mergent\n",
    "\n",
    "#10 Remove bonds lacking information for accrued interest (and hence returns)\n",
    "fisd['offering_date']            = pd.to_datetime(fisd['offering_date'], format='%Y-%m-%d')\n",
    "fisd['dated_date']               = pd.to_datetime(fisd['dated_date'],    format='%Y-%m-%d')\n",
    "fisd['maturity']                 = pd.to_datetime(fisd['maturity'],      format='%Y-%m-%d')\n",
    "\n",
    "# 10.1 Dated date\n",
    "fisd = fisd[~fisd.dated_date.isnull()]\n",
    "# 10.2 Interest frequency\n",
    "fisd = fisd[~fisd.interest_frequency.isnull()]\n",
    "# 10.3 Day count basis\n",
    "fisd = fisd[~fisd.day_count_basis.isnull()]\n",
    "# 10.4 Offering date\n",
    "fisd = fisd[~fisd.offering_date.isnull()]\n",
    "# 10.5 Coupon type\n",
    "fisd = fisd[~fisd.coupon_type.isnull()]\n",
    "# 10.6 Coupon value\n",
    "fisd = fisd[~fisd.coupon.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '..'/ Path(DATA_DIR) / \"pulled\" / \"fisd.csv\"\n",
    "path.parent.mkdir(parents=True, exist_ok=True)\n",
    "fisd.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ensure KPP Bonds are in the sample\n",
    "\n",
    "These are the bonds add mannually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs_KPP = pd.read_csv('..'/ Path(DATA_DIR) / \"manual\" / \"cusips.csv\")\n",
    "IDs_KPP.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "IDs_KPP.columns = ['complete_cusip']\n",
    "IDs_KPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = fisd[['complete_cusip']]\n",
    "IDs = pd.concat([IDs, IDs_KPP], axis = 0)\n",
    "\n",
    "#* Ensure IDs unique\n",
    "IDS = IDs.drop_duplicates(subset='complete_cusip')  # IDS not used in this file after this line???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parse out bonds for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Break into chunks for WRDS\n",
    "CUSIP_Sample = list( fisd['complete_cusip'].unique() )\n",
    "def divide_chunks(l, n): \t\n",
    "\t# looping till length l \n",
    "\tfor i in range(0, len(l), n): \n",
    "\t\tyield l[i:i + n] \n",
    "\n",
    "cusip_chunks  = list(divide_chunks(CUSIP_Sample, 500))\n",
    "\n",
    "print(f'total bonds:{len(CUSIP_Sample)}, divided into {len(cusip_chunks)} chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Process intraday data to daily data\n",
    "Since the total bonds are too large to transform, we will process data in chunks. Here, we use the first chunk as an example to showcase the steps to transform intraday data into daily price, volume and volume-weighted price data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CleaningExport   = pd.DataFrame( index   = range(0,len(cusip_chunks)),\n",
    "                               columns = ['Obs.Pre',\n",
    "                                          'Obs.PostBBW',\n",
    "                                          'Obs.PostDickNielsen'])\n",
    "\n",
    "price_super_list       = []\n",
    "volume_super_list      = []\n",
    "illiquidity_super_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "tempList = cusip_chunks[i]    \n",
    "tempTuple = tuple(tempList)\n",
    "parm = {'cusip_id': (tempTuple)}\n",
    "\n",
    "trace = db.raw_sql(\"SELECT cusip_id,bond_sym_id,trd_exctn_dt,trd_exctn_tm,days_to_sttl_ct,lckd_in_ind,wis_fl,sale_cndtn_cd,msg_seq_nb, trc_st, trd_rpt_dt,trd_rpt_tm, entrd_vol_qt, rptd_pr,yld_pt,asof_cd,orig_msg_seq_nb,rpt_side_cd,cntra_mp_id FROM trace.trace_enhanced WHERE cusip_id in %(cusip_id)s\", \n",
    "                  params=parm)\n",
    "trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CleaningExport['Obs.Pre'].iloc[i] = int(len(trace))\n",
    "    \n",
    "#### Basically try-catch --> ensure >100 obs in the pulled data, handles\n",
    "#### edge cases where there is not any data     \n",
    "if len(trace) <= 100:\n",
    "    CleaningExport['Obs.PostBBW'].iloc[i] = int(len(trace))\n",
    "    CleaningExport['Obs.PostDickNielsen'].iloc[i] = int(len(trace))\n",
    "\n",
    "else:\n",
    "    \n",
    "    # Convert dates to datetime        \n",
    "    trace['trd_exctn_dt']         = pd.to_datetime(trace['trd_exctn_dt'], format = '%Y-%m-%d') # execution date\n",
    "    trace['trd_rpt_dt']           = pd.to_datetime(trace['trd_rpt_dt'],   format = '%Y-%m-%d') # report date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Variable Handling\n",
    "- Convert Settlement indicator,when-issued indicator, locked-in indicator, sale condition indicator to string.\n",
    "- Remove trades with volume < $10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Settlement indicator to string     \n",
    "trace['days_to_sttl_ct'] = trace['days_to_sttl_ct'].astype('str')                   \n",
    "\n",
    "# Convert when-issued indicator to string    \n",
    "trace['wis_fl'] = trace['wis_fl'].astype('str')  # When Issued Indicator\n",
    "\n",
    "# Convert locked-in indicator to string    \n",
    "trace['lckd_in_ind'] = trace['lckd_in_ind'].astype('str') #Locked In Indicator\n",
    "\n",
    "# Convert sale condition indicator to string    \n",
    "trace['sale_cndtn_cd'] = trace['sale_cndtn_cd'].astype('str') \n",
    "                                            \n",
    "# Remove trades with volume < $10,000\n",
    "trace = trace[ (trace['entrd_vol_qt']) >= 10000  ]   # Quantity\n",
    "                \n",
    "CleaningExport['Obs.PostBBW'].iloc[i] = int(len(trace)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Parsing out Post 2012/02/06 Data\n",
    "The data has different format before and after 2012/02/06 in WRDS. So need to deal with the data seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* ************************************ */\n",
    "#* 1.0 Parsing out Post 2012/02/06 Data */\n",
    "#* ************************************ */              \n",
    "post= trace[(trace['cusip_id'] != '') & (trace['trd_rpt_dt'] >=\"2012-02-06\")]\n",
    "pre = trace[(trace['cusip_id'] != '') & (trace['trd_rpt_dt'] < \"2012-02-06\")]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Post 2012/02/06 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* ************************************** */\n",
    "#* 1.1 Remove Cancellation and Correction */\n",
    "#* ************************************** */        \n",
    "\n",
    "# * Match Cancellation and Correction using following 7 keys:\n",
    "# * Cusip_id, Execution Date and Time, Quantity, Price, Buy/Sell Indicator, Contra Party\n",
    "# * C and X records show the same MSG_SEQ_NB as the original record;\n",
    "        \n",
    "post_tr = post[(post['trc_st'] == 'T') | (post['trc_st'] == 'R')]   # Trade Status: report, new correction\n",
    "post_xc = post[(post['trc_st'] == 'X') | (post['trc_st'] == 'C')]   # cancel\n",
    "post_y  = post[(post['trc_st'] == 'Y')]   # reversal\n",
    "\n",
    "_clean_post1 = pd.merge(post_tr.drop_duplicates(), post_xc[[\n",
    "                          'cusip_id',        # 1\n",
    "                          'trd_exctn_dt',    # 2\n",
    "                          'trd_exctn_tm',    # 3\n",
    "                          'rptd_pr',         # 4 price\n",
    "                          'entrd_vol_qt',    # 5 volume\n",
    "                          'rpt_side_cd',     # 6 side\n",
    "                          'cntra_mp_id',     # 7 Contra party indicator\n",
    "                          'msg_seq_nb',      # 8 reference number\n",
    "                          'trc_st']],    \n",
    "                           left_on=[\n",
    "                              'cusip_id',        # 1\n",
    "                              'trd_exctn_dt',    # 2\n",
    "                              'trd_exctn_tm',    # 3\n",
    "                              'rptd_pr',         # 4\n",
    "                              'entrd_vol_qt',    # 5\n",
    "                              'rpt_side_cd',     # 6\n",
    "                              'cntra_mp_id',     # 7\n",
    "                              'msg_seq_nb'],     # 8\n",
    "                                          right_on=[ \n",
    "                                         'cusip_id',          # 1\n",
    "                                           'trd_exctn_dt',    # 2\n",
    "                                           'trd_exctn_tm',    # 3\n",
    "                                           'rptd_pr',         # 4\n",
    "                                           'entrd_vol_qt',    # 5\n",
    "                                           'rpt_side_cd',     # 6\n",
    "                                           'cntra_mp_id',     # 7\n",
    "                                           'msg_seq_nb'],     # 8                       \n",
    "                        how = \"left\")\n",
    "\n",
    "# Remove the matched \"Trade Report\" observations;\n",
    "clean_post1 = _clean_post1[_clean_post1['trc_st_y'].isnull()]\n",
    "\n",
    "# Clean-up clean_post1#\n",
    "clean_post1.drop(['trc_st_y'], axis = 1, inplace = True)\n",
    "clean_post1.rename(columns={'trc_st_x':'trc_st'}, inplace=True) \n",
    "\n",
    "                \n",
    "#* ******************** */\n",
    "#* 1.2 Remove Reversals */\n",
    "#* ******************** */\n",
    "\n",
    "# * Match Reversal using the same 7 keys:\n",
    "# * Cusip_id, Execution Date and Time, Quantity, Price, Buy/Sell Indicator, Contra Party\n",
    "# * R records show ORIG_MSG_SEQ_NB matching orignal record MSG_SEQ_NB;\n",
    "_clean_post2 = pd.merge(_clean_post1.drop_duplicates(), post_y[[\n",
    "                  'cusip_id',        # 1\n",
    "                  'trd_exctn_dt',    # 2\n",
    "                  'trd_exctn_tm',    # 3\n",
    "                  'rptd_pr',         # 4\n",
    "                  'entrd_vol_qt',    # 5\n",
    "                  'rpt_side_cd',     # 6\n",
    "                  'cntra_mp_id',     # 7\n",
    "                  'orig_msg_seq_nb', # 8\n",
    "                  'trc_st']],    \n",
    "                    left_on=[\n",
    "                      'cusip_id',        # 1\n",
    "                      'trd_exctn_dt',    # 2\n",
    "                      'trd_exctn_tm',    # 3\n",
    "                      'rptd_pr',         # 4\n",
    "                      'entrd_vol_qt',    # 5\n",
    "                      'rpt_side_cd',     # 6\n",
    "                      'cntra_mp_id',     # 7\n",
    "                      'msg_seq_nb'],     # 8\n",
    "                                  right_on=[ \n",
    "                                  'cusip_id',          # 1\n",
    "                                    'trd_exctn_dt',    # 2\n",
    "                                    'trd_exctn_tm',    # 3\n",
    "                                    'rptd_pr',         # 4\n",
    "                                    'entrd_vol_qt',    # 5\n",
    "                                    'rpt_side_cd',     # 6\n",
    "                                    'cntra_mp_id',     # 7\n",
    "                                    'orig_msg_seq_nb'],# 8                       \n",
    "                how = \"left\")\n",
    "                      \n",
    "# Remove the matched \"Trade Report\" observations;\n",
    "clean_post2 = _clean_post2[_clean_post2['trc_st_y'].isnull()].drop_duplicates()\n",
    "                \n",
    "# Clean-up clean_post1#\n",
    "clean_post2.drop(['orig_msg_seq_nb_y','trc_st_y','trc_st'], axis = 1, inplace = True)\n",
    "clean_post2.rename(columns={'orig_msg_seq_nb_x':'orig_msg_seq_nb',\n",
    "                            'trc_st_x':'trc_st'}, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Pre 2012-02-06 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* ************************************ */\n",
    "#*  van Binsbergen, Nozawa, and Schwert */\n",
    "#*  We restrict the bond transactions in */\n",
    "#*  our sample by removing those that are */\n",
    "#*  whenissued, have special conditions, are  */\n",
    "#*  locked in, and have days-to-settlement  */\n",
    "#*  of more than two */\n",
    "#*  days in the pre-2012 database */\n",
    "#* ************************************ */     \n",
    "        \n",
    "# Remove trades with > 2-days to settlement #\n",
    "# Keep all with days_to_sttl_ct equal to None, 000, 001 or 002\n",
    "pre = pre[   (pre['days_to_sttl_ct'] == '002') | (pre['days_to_sttl_ct'] == '000')\\\n",
    "            | (pre['days_to_sttl_ct']  == '001') | (pre['days_to_sttl_ct'] == 'None') ]\n",
    "\n",
    "# Remove when-issued indicator #\n",
    "pre = pre[  (pre['wis_fl'] != 'Y')        ]  \n",
    "\n",
    "# Remove locked-in indicator #\n",
    "pre = pre[  (pre['lckd_in_ind'] != 'Y')   ]\n",
    "\n",
    "# Remove trades with special conditions #\n",
    "pre = pre[  (pre['sale_cndtn_cd'] == 'None') | (pre['sale_cndtn_cd'] == '@')   ]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* ********************************* */\n",
    "#* 2.1 Remove Cancellation Cases (C) */\n",
    "#* ********************************* */\n",
    "pre_c = pre[pre['trc_st'] == 'C']       \n",
    "pre_w = pre[pre['trc_st'] == 'W']       \n",
    "pre_t = pre[pre['trc_st'] == 'T']     \n",
    "\n",
    "# Match Cancellation by the 7 keys:\n",
    "# Cusip_ID, Execution Date and Time, Quantity, Price, Buy/Sell Indicator, Contra Party\n",
    "# C records show ORIG_MSG_SEQ_NB matching orignal record MSG_SEQ_NB;\n",
    "merged = pd.merge(pre_t.drop_duplicates(), pre_c[[\n",
    "                    'cusip_id',\n",
    "                    'trd_exctn_dt',\n",
    "                    'trd_exctn_tm',\n",
    "                    'rptd_pr',\n",
    "                    'entrd_vol_qt',\n",
    "                    'trd_rpt_dt',\n",
    "                    'orig_msg_seq_nb',\n",
    "                    'trc_st']],\n",
    "                    left_on=[        'cusip_id', \n",
    "                                    'trd_exctn_dt', \n",
    "                                    'trd_exctn_tm', \n",
    "                                    'rptd_pr', \n",
    "                                    'entrd_vol_qt', \n",
    "                                    'trd_rpt_dt', \n",
    "                                    'msg_seq_nb'], # msg\n",
    "                                    right_on=['cusip_id', \n",
    "                                    'trd_exctn_dt', \n",
    "                                    'trd_exctn_tm', \n",
    "                                    'rptd_pr', \n",
    "                                    'entrd_vol_qt', \n",
    "                                    'trd_rpt_dt', \n",
    "                                    'orig_msg_seq_nb']  ,  # orig_msg                      \n",
    "                how = \"left\")\n",
    "\n",
    "\n",
    "merged = merged.drop_duplicates()\n",
    "# Filter out C cases\n",
    "_del_c     = merged[merged['trc_st_y'] == 'C']\n",
    "clean_pre1 = merged[merged['trc_st_y'] != 'C']\n",
    "\n",
    "# Clean-up clean_pre1#\n",
    "clean_pre1.drop(['orig_msg_seq_nb_y', 'trc_st_y'], axis = 1, inplace = True)\n",
    "clean_pre1.rename(columns={'trc_st_x':'trc_st',\n",
    "                            'orig_msg_seq_nb_x':'orig_msg_seq_nb'}, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* ******************************* */\n",
    "#* 2.2 Remove Correction Cases (W) */\n",
    "#* ******************************* */\n",
    "\n",
    "# * NOTE: on a given day, a bond can have more than one round of correction\n",
    "# * One W to correct an older W, which then corrects the original T\n",
    "# * Before joining back to the T data, first need to clean out the W to\n",
    "# * handle the situation described above;\n",
    "# * The following section handles the chain of W cases;\n",
    "\n",
    "# 2.2.1 Sort out all msg_seq_nb;\n",
    "w_msg = pre_w[['cusip_id', 'bond_sym_id', 'trd_exctn_dt', 'trd_exctn_tm', 'msg_seq_nb']]\n",
    "w_msg['flag'] = 'msg'\n",
    "\n",
    "# 2.2.1 Sort out all mapped original msg_seq_nb;\n",
    "w_omsg = pre_w[['cusip_id', 'bond_sym_id', 'trd_exctn_dt', 'trd_exctn_tm', 'orig_msg_seq_nb']]\n",
    "w_omsg = w_omsg.rename(columns={'orig_msg_seq_nb': 'msg_seq_nb'})\n",
    "w_omsg['flag'] = 'omsg'\n",
    "\n",
    "w = pd.concat([w_omsg, w_msg])\n",
    "\n",
    "# 2.2.2 Count the number of appearance (napp) of a msg_seq_nb: \n",
    "w_napp = w.groupby(['cusip_id', \n",
    "                    'bond_sym_id', \n",
    "                    'trd_exctn_dt', \n",
    "                    'trd_exctn_tm', \n",
    "                    'msg_seq_nb']).size().reset_index(name='napp')\n",
    "\n",
    "# * 2.2.3 Check whether one msg_seq_nb is associated with both msg and orig_msg or only to orig_msg;\n",
    "# * If msg_seq_nb appearing more than once is associated with only orig_msg - \n",
    "# * It means that more than one msg_seq_nb is linked to the same orig_msg_seq_nb for correction. \n",
    "# * Examples: cusip_id='362320AX1' and trd_Exctn_dt='04FEB2005'd (3 cases like this in total)\n",
    "# * If ntype=2 then a msg_seq_nb is associated with being both msg_seq_nb and orig_msg_seq_nb;\n",
    "\n",
    "w_mult = w.drop_duplicates(subset = [ 'cusip_id', \n",
    "                                        'bond_sym_id', \n",
    "                                        'trd_exctn_dt', \n",
    "                                        'trd_exctn_tm', \n",
    "                                        'msg_seq_nb', \n",
    "                                        'flag'])\n",
    "\n",
    "\n",
    "w_mult1 = w_mult.groupby(['cusip_id', \n",
    "                            'bond_sym_id', \n",
    "                            'trd_exctn_dt', \n",
    "                            'trd_exctn_tm', \n",
    "                            'msg_seq_nb',\n",
    "                            ]).size().reset_index(name='ntype')\n",
    "\n",
    "# 2.2.4 Combine the npair and ntype info;       \n",
    "w_comb = pd.merge(w_napp, w_mult1, on=['cusip_id', \n",
    "                                        'bond_sym_id', \n",
    "                                        'trd_exctn_dt', \n",
    "                                        'trd_exctn_tm', \n",
    "                                        'msg_seq_nb'], \n",
    "            how='left').sort_values(by= ['cusip_id',  \n",
    "                                            'trd_exctn_dt', \n",
    "                                            'trd_exctn_tm'])\n",
    "# Map back by matching CUSIP Excution Date and Time to remove msg_seq_nb that appears more than once;\n",
    "# If napp=1 or (napp>1 but ntype=1);\n",
    "__w_keep = pd.merge(w_comb[(w_comb['napp'] == 1) | ((w_comb['napp'] > 1) & (w_comb['ntype'] == 1))], \n",
    "                    w, \n",
    "                    on=['cusip_id',                               \n",
    "                        'trd_exctn_dt', \n",
    "                        'trd_exctn_tm', \n",
    "                        'msg_seq_nb',\n",
    "                        ], \n",
    "    how = \"inner\",\n",
    "    suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)').sort_values(by=\n",
    "                            ['cusip_id',                               \n",
    "                            'trd_exctn_dt', \n",
    "                            'trd_exctn_tm'])\n",
    "\n",
    "__w_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "\n",
    "# 2.2.5 Caluclate no of pair of records;\n",
    "# Assuming the original table is named \"__w_keep\"\n",
    "\n",
    "__w_keep['npair'] = __w_keep.drop_duplicates().groupby(by=[\n",
    "                                            'cusip_id', \n",
    "                                            'trd_exctn_dt',\n",
    "                                            'trd_exctn_tm'])['cusip_id'].transform(\"count\")/2\n",
    "__w_keep =  __w_keep.sort_values(by=\n",
    "                            ['cusip_id',                               \n",
    "                            'trd_exctn_dt', \n",
    "                            'trd_exctn_tm'])\n",
    "\n",
    "# For records with only one pair of entry at a given time stamp \n",
    "# - transpose using the flag information;\n",
    "__w_keep1 = __w_keep[__w_keep['npair']==1].pivot(index=['cusip_id', \n",
    "                                                        'trd_exctn_dt', \n",
    "                                                        'trd_exctn_tm',\n",
    "                                                        ],\n",
    "                                                    columns='flag', \n",
    "                                                    values='msg_seq_nb')\n",
    "\n",
    "__w_keep1.reset_index(inplace=True)\n",
    "__w_keep1.rename(columns={'msg': 'msg_seq_nb', 'omsg': 'orig_msg_seq_nb'}, inplace=True)\n",
    "__w_keep1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For records with more than one pair of entry at a given time stamp \n",
    "# - join back the original msg_seq_nb;\n",
    "__w_keep2 = pd.merge(__w_keep[(__w_keep['flag'] == 'msg') & (__w_keep['npair'] > 1)], pre_w, \n",
    "            left_on = ['cusip_id', 'trd_exctn_dt', 'trd_exctn_tm', 'msg_seq_nb'], \n",
    "            right_on = ['cusip_id', 'trd_exctn_dt', 'trd_exctn_tm', 'msg_seq_nb'], \n",
    "            how = 'left',\n",
    "            suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)').sort_values(by=\n",
    "                                        ['cusip_id',                               \n",
    "                                        'trd_exctn_dt', \n",
    "                                        'trd_exctn_tm'])\n",
    "\n",
    "__w_keep2 = __w_keep2[['cusip_id',                             \n",
    "                        'trd_exctn_dt',\n",
    "                        'trd_exctn_tm',\n",
    "                        'msg_seq_nb',\n",
    "                        'orig_msg_seq_nb']].drop_duplicates()\n",
    "\n",
    "__w_clean = pd.concat([__w_keep1, __w_keep2], axis=0)\n",
    "\n",
    "# * 2.2.6 Join back to get all the other information;\n",
    "w_clean = pd.merge(__w_clean, pre_w.drop(columns = ['orig_msg_seq_nb']), \n",
    "            left_on  = ['cusip_id', 'trd_exctn_dt', 'trd_exctn_tm', 'msg_seq_nb'], \n",
    "            right_on = ['cusip_id', 'trd_exctn_dt', 'trd_exctn_tm', 'msg_seq_nb'], \n",
    "            how = 'left').drop_duplicates(subset = ['orig_msg_seq_nb',\n",
    "                                                    'cusip_id',\n",
    "                                                    'trd_exctn_dt',\n",
    "                                                    'trd_exctn_tm',\n",
    "                                                    'msg_seq_nb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /* 2.2.7 Match up with Trade Record data to delete the matched T record */;\n",
    "# * Matching by Cusip_ID, Date, and MSG_SEQ_NB;\n",
    "# * W records show ORIG_MSG_SEQ_NB matching orignal record MSG_SEQ_NB;\n",
    "clean_pre2 = pd.merge(clean_pre1.drop_duplicates(), w_clean[[\n",
    "                                            'cusip_id', \n",
    "                                            'trd_exctn_dt', \n",
    "                                            'msg_seq_nb',\n",
    "                                            'orig_msg_seq_nb',\n",
    "                                            'trc_st' ]], \n",
    "                left_on  = ['cusip_id', 'trd_exctn_dt', 'msg_seq_nb'], \n",
    "                right_on = ['cusip_id', 'trd_exctn_dt', 'orig_msg_seq_nb'], \n",
    "                how = 'left')\n",
    "        \n",
    "# Clean-up clean_pre2 #\n",
    "clean_pre2.rename(columns={ 'trc_st_x':'trc_st',\n",
    "                            'trc_st_y':'trc_st_w',\n",
    "                            'msg_seq_nb_x':'msg_seq_nb',\n",
    "                            'msg_seq_nb_y':'mod_msg_seq_nb',\n",
    "                            'orig_msg_seq_nb_x':'orig_msg_seq_nb',\n",
    "                            'orig_msg_seq_nb_y':'mod_orig_msg_seq_nb'}, inplace=True) \n",
    "\n",
    "_del_w =  clean_pre2[clean_pre2.trc_st_w == \"W\"]                                                                              \n",
    "\n",
    "# * Delete matched T records;\n",
    "_clean_pre2 =  clean_pre2[clean_pre2['trc_st_w'].isnull()]\n",
    "                \n",
    "_clean_pre2 = _clean_pre2.drop(columns = ['trc_st_w', \n",
    "                                            'mod_msg_seq_nb', \n",
    "                                            'mod_orig_msg_seq_nb'])\n",
    "        \n",
    "# * Replace T records with corresponding W records;\n",
    "# * Filter out W records with valid matching T from the previous step;\n",
    "\n",
    "rep_w = pd.merge(w_clean.drop_duplicates(), _del_w[['cusip_id',\n",
    "                                                    'trd_exctn_dt',\n",
    "                                                    'trc_st_w',\n",
    "                                                    'mod_msg_seq_nb',\n",
    "                                                    'mod_orig_msg_seq_nb']], \n",
    "            left_on  = ['cusip_id', 'trd_exctn_dt', 'msg_seq_nb'], \n",
    "            right_on = ['cusip_id', 'trd_exctn_dt', 'mod_msg_seq_nb'], \n",
    "        how = 'left')\n",
    "\n",
    "\n",
    "rep_w = rep_w[rep_w['trc_st_w'] == 'W']\n",
    "\n",
    "rep_w = rep_w.drop_duplicates(subset = ['cusip_id',\n",
    "                                        'trd_exctn_dt',\n",
    "                                        'msg_seq_nb',\n",
    "                                        'orig_msg_seq_nb',\n",
    "                                        'rptd_pr',\n",
    "                                        'entrd_vol_qt'])\n",
    "rep_w = rep_w.drop(columns = [            'trc_st_w', \n",
    "                                            'mod_msg_seq_nb', \n",
    "                                            'mod_orig_msg_seq_nb'])\n",
    "        \n",
    "clean_pre3 = pd.concat([_clean_pre2, rep_w], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* ***************** */\n",
    "#* 2.3 Reversal Case */\n",
    "#* ***************** */\n",
    "# Filter data by asof_cd = 'R' and keep only certain columns\n",
    "\n",
    "_rev_header = clean_pre3[ clean_pre3['asof_cd'] == 'R'][[  'cusip_id', \n",
    "                                                            'bond_sym_id',\n",
    "                                                            'trd_exctn_dt',\n",
    "                                                            'trd_exctn_tm',\n",
    "                                                            'trd_rpt_dt',\n",
    "                                                            'trd_rpt_tm',\n",
    "                                                            'entrd_vol_qt', \n",
    "                                                            'rptd_pr',\n",
    "                                                            'rpt_side_cd',\n",
    "                                                            'cntra_mp_id']]       \n",
    "#* Option B: Match by only 6 keys: CUSIP_ID, \n",
    "# Execution Date, Vol, Price, B/S and C/D (remove the time dimension);\n",
    "_rev_header = _rev_header.sort_values(by=['cusip_id',\n",
    "                                            'bond_sym_id', \n",
    "                                            'trd_exctn_dt', \n",
    "                                            'entrd_vol_qt', \n",
    "                                            'rptd_pr', \n",
    "                                            'rpt_side_cd',\n",
    "                                            'cntra_mp_id', \n",
    "                                            'trd_exctn_tm', \n",
    "                                            'trd_rpt_dt', \n",
    "                                            'trd_rpt_tm'])\n",
    "        \n",
    "_rev_header6 = _rev_header.copy()\n",
    "_rev_header6['seq'] = _rev_header6.groupby(['cusip_id', \n",
    "                                            'bond_sym_id',\n",
    "                                            'trd_exctn_dt',\n",
    "                                            'entrd_vol_qt',\n",
    "                                            'rptd_pr',\n",
    "                                            'rpt_side_cd', \n",
    "                                            'cntra_mp_id']).cumcount() + 1\n",
    "\n",
    "# * Create the same ordering among the non-reversal records;\n",
    "# * Remove records that are R (reversal) D (Delayed dissemination) and \n",
    "# X (delayed reversal);\n",
    "_clean_pre4 = clean_pre3[~clean_pre3['asof_cd'].isin(['R', 'X', 'D'])]\n",
    "\n",
    "_clean_pre4_header = _clean_pre4[['cusip_id',\n",
    "                                    'bond_sym_id',\n",
    "                                    'trd_exctn_dt',\n",
    "                                    'trd_exctn_tm',\n",
    "                                    'entrd_vol_qt',\n",
    "                                    'rptd_pr', \n",
    "                                    'rpt_side_cd',\n",
    "                                    'cntra_mp_id',\n",
    "                                    'trd_rpt_dt', \n",
    "                                    'trd_rpt_tm', \n",
    "                                    'msg_seq_nb']]\n",
    "\n",
    "# Match by 6 keys (excluding execution time);\n",
    "_clean_pre4_header = _clean_pre4_header.sort_values(by=['cusip_id', \n",
    "                                                        'bond_sym_id',\n",
    "                                                        'trd_exctn_dt',\n",
    "                                                        'entrd_vol_qt',\n",
    "                                                        'rptd_pr', \n",
    "                                                        'rpt_side_cd', \n",
    "                                                        'cntra_mp_id',\n",
    "                                                        'trd_exctn_tm', \n",
    "                                                        'trd_rpt_dt', \n",
    "                                                        'trd_rpt_tm', \n",
    "                                                        'msg_seq_nb'])\n",
    "\n",
    "_clean_pre4_header['seq6'] = _clean_pre4_header.groupby(['cusip_id', \n",
    "                                                        'bond_sym_id', \n",
    "                                                        'trd_exctn_dt', \n",
    "                                                        'entrd_vol_qt',\n",
    "                                                        'rptd_pr', \n",
    "                                                        'rpt_side_cd', \n",
    "                                                        'cntra_mp_id']).cumcount() + 1\n",
    "\n",
    "_clean_pre5_header = pd.merge(_clean_pre4_header.drop_duplicates(), _rev_header6, left_on=['cusip_id',\n",
    "                                                                    'trd_exctn_dt', \n",
    "                                                                    'entrd_vol_qt',\n",
    "                                                                    'rptd_pr', \n",
    "                                                                    'rpt_side_cd', \n",
    "                                                                    'cntra_mp_id',\n",
    "                                                                    'seq6'], \n",
    "                                                                right_on=['cusip_id',\n",
    "                                                                    'trd_exctn_dt', \n",
    "                                                                    'entrd_vol_qt',\n",
    "                                                                    'rptd_pr', \n",
    "                                                                    'rpt_side_cd', \n",
    "                                                                    'cntra_mp_id',\n",
    "                                                                    'seq'],                                    \n",
    "                                                how = \"left\", \n",
    "                                                suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)')\n",
    "                        \n",
    "_clean_pre5_header = _clean_pre5_header.rename(columns={'seq': 'rev_seq6'}).drop_duplicates()\n",
    "_rev_matched6      = _clean_pre5_header[_clean_pre5_header['rev_seq6'].notna()]\n",
    "\n",
    "\n",
    "# As 6 key matching has a higher record of finding reversal match, \n",
    "# use the 6 keys results now;\n",
    "_clean_pre5_header = _clean_pre5_header[_clean_pre5_header['rev_seq6'].isna()]\n",
    "_clean_pre5_header = _clean_pre5_header.drop(columns=['rev_seq6',\n",
    "                                                        'seq6']    )\n",
    "\n",
    "\n",
    "_clean_pre5 = _clean_pre4.merge(_clean_pre5_header, on=['cusip_id',\n",
    "                                                        'trd_exctn_dt',\n",
    "                                                        'trd_exctn_tm',\n",
    "                                                        'entrd_vol_qt', \n",
    "                                                        'rptd_pr', \n",
    "                                                        'rpt_side_cd', \n",
    "                                                        'cntra_mp_id', \n",
    "                                                        'msg_seq_nb', \n",
    "                                                        'trd_rpt_dt',\n",
    "                                                        'trd_rpt_tm'], how='inner',                                      \n",
    "                                suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)')\n",
    "\n",
    "_clean_pre5 = _clean_pre5.drop_duplicates()\n",
    "\n",
    "# =====================================================================\n",
    "# * Combine the pre and post data together */;\n",
    "clean_post2 = clean_post2[[                    'cusip_id',\n",
    "                                                'trd_exctn_dt',                                                    \n",
    "                                                'rptd_pr',\n",
    "                                                'entrd_vol_qt',\n",
    "                                                'rpt_side_cd',\n",
    "                                                ]]\n",
    "_clean_pre5 = _clean_pre5[clean_post2.columns]\n",
    "        \n",
    "trace_post = pd.concat([_clean_pre5, clean_post2], ignore_index=True)\n",
    "\n",
    "trace = trace_post.set_index(['cusip_id','trd_exctn_dt']).sort_index(level = 'cusip_id') \n",
    "\n",
    "CleaningExport['Obs.PostDickNielsen'].iloc[i] = int(len(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Generating Prices and Volume and Illiquidity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* ***************** */\n",
    "#* Prices / Volume   */\n",
    "#* ***************** */\n",
    "# Price - Equal-Weight   #\n",
    "prc_EW = trace.groupby(['cusip_id','trd_exctn_dt'])[['rptd_pr']].mean().sort_index(level  =  'cusip_id').round(4) \n",
    "prc_EW.columns = ['prc_ew']\n",
    "\n",
    "# Price - Volume-Weight # \n",
    "trace['dollar_vol']    = ( trace['entrd_vol_qt'] * trace['rptd_pr']/100 ).round(0) # units x clean prc                               \n",
    "trace['value-weights'] = trace.groupby([ 'cusip_id','trd_exctn_dt'],\n",
    "                                        group_keys=False)[['entrd_vol_qt']].apply( lambda x: x/np.nansum(x) )\n",
    "prc_VW = trace.groupby(['cusip_id','trd_exctn_dt'])[['rptd_pr','value-weights']].apply( lambda x: np.nansum( x['rptd_pr'] * x['value-weights']) ).to_frame().round(4)\n",
    "prc_VW.columns = ['prc_vw']\n",
    "\n",
    "PricesAll = prc_EW.merge(prc_VW, how = \"inner\", left_index = True, right_index = True)  \n",
    "PricesAll.columns                = ['prc_ew','prc_vw']   \n",
    "    \n",
    "# Volume #\n",
    "VolumesAll                        = trace.groupby(['cusip_id','trd_exctn_dt'])[['entrd_vol_qt']].sum().sort_index(level  =  \"cusip_id\")                       \n",
    "VolumesAll['dollar_volume']       = trace.groupby(['cusip_id','trd_exctn_dt'])[['dollar_vol']].sum().sort_index(level  =  \"cusip_id\").round(0)\n",
    "VolumesAll.columns                = ['qvolume','dvolume']      \n",
    "\n",
    "# Illiquidity #\n",
    "# (1) Daily bid prices          #\n",
    "# (2) Daily ask prices          #\n",
    "# (3) Number of daily trades    #\n",
    "\n",
    "# Bid and Ask prices #\n",
    "_bid       = trace[trace['rpt_side_cd'] == 'S']\n",
    "_ask       = trace[trace['rpt_side_cd'] == 'B']\n",
    "\n",
    "# Volume weight Bids #\n",
    "_bid['dollar_vol']    = ( _bid['entrd_vol_qt'] * _bid['rptd_pr']/100 )\\\n",
    "    .round(0) # units x clean prc                               \n",
    "_bid['value-weights'] = _bid.groupby([ 'cusip_id','trd_exctn_dt'],\n",
    "            group_keys=False)[['entrd_vol_qt']]\\\n",
    "    .apply( lambda x: x/np.nansum(x) )\n",
    "\n",
    "prc_BID = _bid.groupby(['cusip_id',\n",
    "                        'trd_exctn_dt'])[['rptd_pr',\n",
    "                                            'value-weights']]\\\n",
    "    .apply( lambda x: np.nansum( x['rptd_pr'] * x['value-weights']) )\\\n",
    "        .to_frame().round(4)\n",
    "        \n",
    "prc_BID.columns = ['prc_bid']\n",
    "\n",
    "# Volume weight Asks #\n",
    "_ask['dollar_vol']    = ( _ask['entrd_vol_qt'] * _ask['rptd_pr']/100 )\\\n",
    "    .round(0) # units x clean prc                               \n",
    "_ask['value-weights'] = _ask.groupby([ 'cusip_id','trd_exctn_dt'],\n",
    "            group_keys=False)[['entrd_vol_qt']]\\\n",
    "    .apply( lambda x: x/np.nansum(x) )\n",
    "\n",
    "prc_ASK = _ask.groupby(['cusip_id',\n",
    "                        'trd_exctn_dt'])[['rptd_pr',\n",
    "                                            'value-weights']]\\\n",
    "    .apply( lambda x: np.nansum( x['rptd_pr'] * x['value-weights']) )\\\n",
    "        .to_frame().round(4)\n",
    "        \n",
    "prc_ASK.columns = ['prc_ask']\n",
    "        \n",
    "prc_BID_ASK =  prc_BID.merge(prc_ASK, \n",
    "                                how = \"inner\", \n",
    "                                left_index = True, \n",
    "                                right_index = True) \n",
    "\n",
    "                                                                                                                                                                                                                                       \n",
    "# =============================================================================          \n",
    "price_super_list.append(PricesAll)      \n",
    "volume_super_list.append(VolumesAll)\n",
    "illiquidity_super_list.append(prc_BID_ASK)\n",
    "# =============================================================================  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_BID_ASK"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
